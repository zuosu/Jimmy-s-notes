## 策略迭代
![](https://files.mdnice.com/user/25190/4d061101-7bd4-4311-b23e-99693fd596f4.png)
step1：策略评估，先给定任意初始值$v_{\pi k}^{(0)}$，对每个状态s迭代求解到最终的$v_{\pi k}$。
step2：策略提升，遍历每一个状态，每一个动作利用上一步求解的$v_{\pi k}$，带入求解$q_{\pi k}(s,a)$，取能够使$q_{\pi k}(s,a)$值最大的动作a，让其做出这个动作的概率变为1从而更新原来的策略。

利用新的策略带入贝尔曼公式重新进行策略评估。
## 举例
![](https://files.mdnice.com/user/25190/e4f0b93a-7e7f-45ee-92cd-a8e2835e2996.png)

蓝色格子代表终止状态，动作空间是左，右，不动。撞到边界得到的-1的惩罚，到达target得到+1的奖励。假设初始策略都为向左运动。

![](https://files.mdnice.com/user/25190/b43a0d6a-0cf3-4aed-8a6c-430ddf3dba1c.png)

上图中的求解状态值函数的封闭形式比较简单所以直接求出，采用迭代的方式去求反而会很麻烦。将求出来得$v_{\pi 0}$代入求得$q_{\pi 0}(s,a)$.

![](https://files.mdnice.com/user/25190/8126af96-a65d-475f-9d26-e43da06d431f.png)

通过寻找最大的$q_{\pi 0}(s,a)$对应的行为a来重新确定策略。从$q_{\pi 0}(s,a)$的表格来看，在$s_1$状态时应选择向右的动作，在$s_2$的状态应选择原地不动的行为。显然经过一次迭代就达到了最优策略。

## 值迭代
![](https://files.mdnice.com/user/25190/dff8abd9-f4b7-475b-bb08-66f755d42256.png)

当$v_k$与$v_{k+1}$还没达到停止迭代的条件时：遍历每一个状态，每一个行为，通过$v_k$计算$q_k(s,a)$，选取$q_k(s,a)$最大的对应的行为对策略进行更新，$v_{k+1}(s)$就是最大$q_k(s,a)$，对值函数进行更新。如此继续计算$v_{k+1}$与$v_{k}$之间的差值是否满足停止条件，循环。

## 举例
![](https://files.mdnice.com/user/25190/d7b3f370-1e87-4536-bbe9-2929c9142a75.png)

橙色代表forbidden area，蓝色代表target。撞墙和到forbidden area的惩罚是-1，达到target的奖励是1。动作空间为$a_1,a_2,a_3,a_4,a_5$

![](https://files.mdnice.com/user/25190/b6e4ecb4-7e87-4564-b921-ff11549b8043.png)

$k=0$时，令$v_0(s_i),i=1,2,3,4$均为0，计算$q_0(s,a)$，选取最大值所对应的行为进行策略更新，如在$s_1$状态时，$a_3$和$a_5$的行为价值函数最大，任意选取一个行为作为当前状态的最优行为，图里选的是$a_5$。同理在$s_2$状态选择$a_3$行为。对应更新完的策略如第一张图中的(b)所示，可以发现此时在$s_2,s_3,s_4$状态的策略都已经达到了最优。

值更新：选取最优行为对应的行为价值函数的值作为新的值函数$v_{1}$。

![](https://files.mdnice.com/user/25190/e024a522-be1a-413a-a4b9-4904ebdeede6.png)

将上面算出来的$v_1(s_i),i=1,2,3,4$代入求解$q_1(s,a)$，同样选取最大的$q_1(s,a)$对应的行为$a$对策略进行更新。最后更新的策略如第一张图中的(c)所示，可以发现每个状态下的动作都是最优的，即此时已经迭代到了最优策略。