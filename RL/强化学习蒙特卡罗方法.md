# 强化学习蒙特卡罗方法
![](https://files.mdnice.com/user/25190/5d2d7c16-029c-4deb-ac3c-804384fb16fa.png)
怎么将$\epsilon-greedy$融入到蒙特卡洛的强化学习算法中？
![](https://files.mdnice.com/user/25190/245bffbe-428c-4e8a-8461-826d95ac1e95.png)

![](https://files.mdnice.com/user/25190/3e83971b-2d7c-49da-bfef-7a3fd6143862.png)
伪代码实现
![](https://files.mdnice.com/user/25190/1b2c64cc-fb11-4f57-90a2-43863cd85051.png)
用了every-visit充分利用了每一目的数据
# 蒙特卡洛算法的例子
## 探索性（Exploration ability）
$\epsilon$很大时，探索性很强
![](https://files.mdnice.com/user/25190/c5816d40-5069-4d0c-bded-9efd8eba10bc.png)
$\epsilon$很小时，探索性很差

![](https://files.mdnice.com/user/25190/d28cbed7-4c02-467f-a3a6-9a62f1f80985.png)
从d图可以看出访问的次数很不均匀

使用一个episode估计
一个episode有100万步，每次迭代都使用上次策略生成的一个包含100万步的episode

![](https://files.mdnice.com/user/25190/cde5c8cb-e7ef-4000-a12d-72495d4b46b6.png)

可以发现经过两次迭代就获得了最优策略。

![](https://files.mdnice.com/user/25190/0e153318-ca2b-44f1-99bb-7b27e163eb03.png)

## 最优性

![](https://files.mdnice.com/user/25190/a2755898-804b-4ad7-a04f-e45ce4b7f7b2.png)

$\epsilon=0$和$\epsilon=0.1$时得到的最优策略是一样的，这被称为一致性（consistency）。但是状态的值随着$\epsilon$增大是逐渐减小的，这说明最优性变差。随着$\epsilon$增大最终的最优策略也不再具有一致性。





