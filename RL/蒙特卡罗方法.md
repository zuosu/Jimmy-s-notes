![](https://files.mdnice.com/user/25190/7c682e59-ccb2-43e8-a6d1-08ced46d2ffd.png)

蒙特卡罗是model-free的方法，和policy iteration（基于model的方法）类似，只不过将policy iteration中的模型用统计的方法实现出来。在进行策略评估时，使用所有目的回报均值来计算$q_{\pi k}(s,a)$

![](https://files.mdnice.com/user/25190/052d3fb0-0b47-4292-b094-d326a65a67e5.png)

## 举例1
![](https://files.mdnice.com/user/25190/055bc1ab-ebe2-4db9-b2ce-0928facd5e32.png)

![](https://files.mdnice.com/user/25190/ed73d0d4-78f0-40b7-8b8a-7db03caacc50.png)

![](https://files.mdnice.com/user/25190/5c3b8c5e-f5f7-430a-a163-ee3a1fc0ebd2.png)

![](https://files.mdnice.com/user/25190/565574bd-09a5-432a-92c4-723d3b2625a1.png)

![](https://files.mdnice.com/user/25190/ab98578e-5a49-4e63-809f-40bbc401df07.png)
## 举例2
![](https://files.mdnice.com/user/25190/2366bb4d-fa65-48db-9932-a64e8b9e7514.png)

每一目的长度变化的影响，长度很短时只有靠近目标的位置会呈现出最优的策略，随着长度变长，离目标位置远的位置也会呈现出最优的策略。

![](https://files.mdnice.com/user/25190/27aab189-0a82-43ef-afae-1fbf92736682.png)

![](https://files.mdnice.com/user/25190/df877437-2033-4d68-81ff-a4d45db4d912.png)