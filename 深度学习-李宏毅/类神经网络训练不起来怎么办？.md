优化失败的原因是梯度接近为0，而导致梯度接近为0有两种情形。
1. 达到了局部极小点
2. 达到了鞍点

这两个点称为critical point。

![](https://files.mdnice.com/user/25190/60568be9-5a16-4656-a438-02b7af8cf263.png)

怎么去判断是到了局部极小点还是到了鞍点？
到达局部极小点时，四周的值都比当前的值大，导致无路可走。而到达鞍点时，有比当前值小的情况，可以逃离鞍点。

![](https://files.mdnice.com/user/25190/70014fc1-18d8-43fc-b8c4-a8655251b5e4.png)

![](https://files.mdnice.com/user/25190/d560121b-cf16-43e9-b2bc-50b8923bb518.png)

使用hessian矩阵可以判断出是局部极小点、局部极大点和鞍点。
1. hessian矩阵正定（特征值eigen values全为正）可以断定为局部极小点。
2. hessian矩阵负定（特征值eigen values全为负）可以断定为局部极大点。
3. hessian矩阵不定（特征值eigen values有正有负）可以断定为局部鞍点。

![](https://files.mdnice.com/user/25190/76705b4f-60f1-449d-9b95-a51beb0a8ccf.png)

举例：

![](https://files.mdnice.com/user/25190/e1c081f7-9480-449c-9cfd-e977b47100c9.png)

![](https://files.mdnice.com/user/25190/f31fcfd3-2718-46d1-87a6-18bd83b42041.png)

怎样逃离鞍点？

![](https://files.mdnice.com/user/25190/3c7c9a16-ef6b-44b0-b2e6-244956264465.png)

![](https://files.mdnice.com/user/25190/e76a2ceb-556e-439f-ba8e-c561b6728837.png)

但在实际当中，这种方法并不可取，因为要算hessian矩阵，计算量较大。局部极小点在平时并不常见。

![](https://files.mdnice.com/user/25190/dc4a1bf3-06d0-427a-8b2f-38fa90697577.png)

![](https://files.mdnice.com/user/25190/c5b2c0f0-c2e8-47c0-a073-cb2043d13c17.png)


![](https://files.mdnice.com/user/25190/d8d20671-634f-48db-a40f-98584108201f.png)


## batch和momentum
### batch的优化

![](https://files.mdnice.com/user/25190/efe0b4ae-eff8-45a3-b293-02cdc43e660e.png)

洗牌（shuffle）是指每次epoch的batch都不一样。


![](https://files.mdnice.com/user/25190/d57f2c7c-52c3-4ea5-8333-87fcb99b6d64.png)

![](https://files.mdnice.com/user/25190/6d9d5faf-090b-4a7e-8b3c-0b86756f1391.png)
因为GPU有平行运算的能力，batch size在1-1000时花费的时间是差不多的，但是若size过大就会费很多时间。

![](https://files.mdnice.com/user/25190/c65839c7-f317-4d46-8787-fc8971a4fc31.png)

若batch size选择的太少，一次epoch需要的时间反而较长。但是小的batch size在训练时可以获得比较好的结果。大的batch size表现得不如小的batch size好，是因为大的batch size中优化有问题。
![](https://files.mdnice.com/user/25190/f31406ae-3fb6-49af-87ac-2bb497f2cdb2.png)

> 为什么小的batch size可以获得比较好的结果？
___
![](https://files.mdnice.com/user/25190/03e8a5a0-d6f5-4c2b-95de-260c3dd3ffac.png)

使用full batch在训练时会卡住，small batch每个batch中的数据不同loss function也不同，这一次在这个损失函数中卡住了，不代表在其他的损失函数会卡住。
___

小的batch size在测试数据中的效果要比大的batch size好，因为此时在训练数据中已经让大的batch size和小的batch size训练的一样好了，而大的batch size在测试数据中表现得并不好，那么造成这种状况的原因是出现了过拟合。

![](https://files.mdnice.com/user/25190/cc855967-2ddf-42f1-93bd-93674c0c91c8.png)

> 怎么去解释小的batch size在测试数据中的效果要比大的batch size好呢？
___

![](https://files.mdnice.com/user/25190/8b293229-8642-4aca-93c2-0e2c752a35a4.png)

大的batch size容易进入sharp Minima而跳不出去，而小的batch size容易跳出，从而进入Flat Minima使预测的结果和真实值相差不大。
___

![](https://files.mdnice.com/user/25190/62a02756-460c-4ea3-b753-7e27d8c274b8.png)

![](https://files.mdnice.com/user/25190/b3d6654e-b5b7-492e-9ccb-c857199edec1.png)
### momentum（动量）
考虑在物理中小球滚下会因为惯性越过局部极小点从而有机会找到最优点。

![](https://files.mdnice.com/user/25190/78e4093d-7127-43a7-98f3-8b5525c0ef2a.png)

momentum是什么？先看一下单纯的梯度下降方法，它每次迭代是沿着梯度的反方向。

![](https://files.mdnice.com/user/25190/204c55e5-2440-4fc6-bd8b-9945870391d8.png)

考虑momentum后，每次迭代会沿着梯度下降反方向和上次迭代方向（即考虑了惯性）的矢量和。

![](https://files.mdnice.com/user/25190/be30feea-5092-4382-803a-5b123f26ca51.png)

![](https://files.mdnice.com/user/25190/8c76b918-3e99-4ba8-902b-b884355247e4.png)

![](https://files.mdnice.com/user/25190/fcb9901b-5d4d-4222-9800-97aedf8af2cf.png)

## 总结

![](https://files.mdnice.com/user/25190/e3f0ead1-bc93-4077-ac96-032036d926b8.png)

## adaptive learning rate

在训练时若发现loss不能再下降了，此时不一定是因为到达了critical point。

![](https://files.mdnice.com/user/25190/250aa187-0b81-4d0a-a95d-d71515c2e1c1.png)

固定的learning rate太大或太小都不容易达到最优点。

![](https://files.mdnice.com/user/25190/2edfd2e0-23e2-4cc6-afa7-590920ec52a1.png)

在error surface中的平坦的方向我们希望learning rate偏大一些，在陡峭的方向希望learning rate小一些。引入参数$\sigma_i^t$。

![](https://files.mdnice.com/user/25190/8c672082-32c3-44e5-a7a6-359be39ce800.png)

### adagrad方法

![](https://files.mdnice.com/user/25190/1a54d160-5396-421b-a1a4-7d12716ef266.png)

![](https://files.mdnice.com/user/25190/19135107-c73b-4540-b020-e098239a8b5d.png)

根据$\sigma_i^t$的计算方法可以看出，在平滑的地方由于计算出来的梯度小，步长就大；在陡峭的地方由于计算出来的梯度大，步长就小。

### RMSProp

![](https://files.mdnice.com/user/25190/0bd2ecf6-1bd5-4f2f-9940-82b974d2d6e3.png)

在计算$\sigma_i^t$增加了权重$\alpha$。

![](https://files.mdnice.com/user/25190/9fb4b6ce-7778-4861-bca8-dbc9c103e3e3.png)

使用Adagrad反应会比较缓慢（使用的是平均值，权重是一样的），但使用RMSProp可以调节$\alpha$的值，让迭代的反应更加迅速。如到了陡峭的区域，希望立即减小步长，此时可以增大$\sigma_i^t$，即增大当前梯度的权重即$1-\alpha$的值。

![](https://files.mdnice.com/user/25190/2d6f9fbe-b36b-4197-a44b-5c8691062289.png)

### Adam

Adam是常用的方法，使用了RMSProp+momentum。

![](https://files.mdnice.com/user/25190/e4009991-1af1-45e8-9dd9-02b1df3c3293.png)

### 效果
![](https://files.mdnice.com/user/25190/63ef5f19-458b-4eef-8c8f-316f9f4813b6.png)

使用Adam会出现红圈圈住的部分？这是因为受到之前所有梯度（包括刚开始梯度比较大的地方）的影响。怎么解决这个问题？

1. learning rate decay

![](https://files.mdnice.com/user/25190/33655024-d9bc-446d-9f4c-b3655fd0f064.png)

随着迭代会接近最优点，在接近最优点时，让learning rate decay此时就不会上下震荡。

2. warm up

warm up在residual network和transformer中都有提及。

![](https://files.mdnice.com/user/25190/f1826fd2-4d5e-4e6d-af72-8ba8fb7a9ad4.png)



![](https://files.mdnice.com/user/25190/da25510b-21f7-47b6-ac73-4aeb36fc0815.png)











