# 周报（时间：220812）
本周主要完成的工作有：  
1. 学习了python中的函数、类，安装了Pytorch，学习了一些数据操作。
2. 整理了《最优化理论与算法》第十一章无约束最优化的直接方法的内容，如下。

# 第十一章 无约束最优化的直接方法

本章介绍解决无约束最优化问题不需要计算导数的方法，这类算法一般称为直接方法。直接方法与使用导数的方法相比，一般来说收敛较慢，但是她对目标函数不要求导数存在，迭代比较简单，编制程序也比较容易，根据数值计算的经验，对于变量不多的问题，能够收到较好的结果。

## 11.1 模式搜索法
### 11.1.2 基本思想

模式搜索法由Hooke和Jeeves提出，因此又被称为Hooke-Jeeves方法。这种方法的基本思想，从几何意义上讲，是寻找较小函数值的“山谷”，力图使迭代产生的序列沿山谷走向逼近极小点。算法从初始基点开始，包括两种类型的移动，这就是探测移动和模式移动。探测移动依次沿n个坐标轴进行，用以确定新的基点和有利于函数值下降的方向。模式移动沿相邻两个基点连线方向进行，试图顺着“山谷”使函数值更快的减小。两种移动交替进行的具体情形如下：


![](https://files.mdnice.com/user/25190/d554f214-80df-4499-8498-fe80bc233a04.png)

设目标函数为$f(x),x\in R^n$，坐标方向
$$
e_j = (0,0,\dots,1,0,\cdots,0)^T,j=1,\cdots,n
$$

给定初始步长$\delta$，加速因子$\alpha$，任取初始点$x_1$作为第一个基点，下面以$x_j$表示第$j$个基点。在每轮探测移动中，自变量用$y_j$表示，即$y_j$是沿$e_j$探测的出发点，这样，$y_1$是沿$e_1$探测的出发点，$y_{n+1}$是沿$e_n$探测得到的点。

首先从$y_1=x_1$出发，进行探测移动，先沿$e_1$探测。如果$f(y_1+\delta e_1)<f(y_1)$，则探测成功，令
$$
y_2 = y_1+\delta e_1
$$
并从$y_2$出发，沿$e_2$进行探测。若沿$e_1$方向探测失败，再沿$-e_1$方向探测，若探测成功则令
$$
y_2 = y_1-\delta e_1
$$
若沿$-e_1$方向探测仍然失败，则令
$$
y_2 = y_1
$$
再从$y_2$沿$e_2$方向探测，方法同上，得到的点记为$y_3$，设沿n个坐标轴方向探测完之后得到的点为$y_{n+1}$。

如果$f(y_{n+1})<f(x_1)$，则$y_{n+1}$作为新的基点，记作
$$
x_2=y_{n+1}
$$
这时，渴望$d=x_2-x_1$是有利于函数值减小的方向。下一步，沿方向$d=x_2-x_1$进行模式移动，令新的$y_1$为$y_1 = x_2+\alpha(x_2-x_1)$。模式移动之后，以$y_1$为起点进行探测移动，这轮探测仍然沿坐标轴方向进行，探测完毕，得到的点记为$y_{n+1}$。如果$f(y_{n+1})<f(x_2)$，则表明此次模式移动成功，于是取新的基点
$$
x_3 = y_{n+1}
$$
再沿方向$x_3-x_2$进行模式搜索。

如果$f(y_{n+1})\ge f(x_2)$，则表明模式移动及此次模式移动之后的的探测移动均无效。于是退回基点$x_2$，减小步长$\delta$，再从$x_2$出发，依次沿各坐标轴方向进行探测移动。如此继续下去，直到满足精度要求，即步长$\delta$小于事先给定的某个小的正数$\epsilon$。

### 11.1.2 计算步骤

![](https://files.mdnice.com/user/25190/309d7f60-becc-4a95-ab7f-7a3b31f5b7d1.png)

例题：

![](https://files.mdnice.com/user/25190/2d819dfc-8a99-436c-ab7e-3ef953cca7ad.png)

```matlab
% Method of Hooke and Jeeves using line search

function [xmin,fmin]=Hooke_Jeeves_Method(x0)

epsilon=1e-5;
x_k=x0;
y=x_k;
k=0;
n=length(x0);
D=eye(n);

while 1
    % 探测搜索
    z_j=y;
    for j=1:n
        d_j=D(:,j);
        lambda_j=fminbnd(@(lambda) theta(lambda,z_j,d_j),-10,10);% 使用精确线搜索确定步长
        z_j=z_j+lambda_j*d_j;
    end
    x_kplus1=z_j;
    
    % stop criteria 
    if (norm(x_kplus1-x_k)<epsilon) && (objective_fun(x_k)-objective_fun(x_kplus1)<epsilon)
        xmin=x_kplus1;
        fmin=objective_fun(xmin);
        k
        return;
    end
    
    % 模式搜索
    d_k=x_kplus1-x_k;
    lambda_bar=fminbnd(@(lambda) theta(lambda,x_kplus1,d_k),-10,10);% 使用精确线搜索确定步长
    y=x_kplus1+lambda_bar*d_k;
    
    % iteration
    x_k=x_kplus1;
    k=k+1;
end
```

目标函数为：

```matlab
function y=objective_fun(x)

y=(1-x(1))^2+5*(x(2)-x(1)^2)^2;
```
用于精确线搜索的theta函数为：

```matlab
function y=theta(lambda,x,d)

y=objective_fun(x+lambda*d);
```
取初始点为$x_1=[2,0]^T$最后在命令行窗口输入：
```matlab
 x0=[2;0];
 [xmin,fmin]=Hooke_Jeeves_Method(x0)

```

得到的结果为
```matlab
k =
     8

xmin =
    1.0000
    1.0000

fmin =
   4.5666e-11
```

可见和解析法求得的结果是一致的。

### 11.1.3 收敛性
1. 不能保证得到的点收敛，但是如果收敛，则一定收敛到目标函数的极小点；
2. 一般认为Hooke-Jeeves方法要比坐标循环法快

## 11.2 Rosenbrock方法

Rosenbrock方法又称为**转轴法**，这种方法与Hooke-Jeeves方法有类似之处，也是顺着“山谷”求函数的极小点。

### 11.2.1 基本思想

Rosenbrock方法每次迭代包括探测阶段和构造搜索方向两部分内容。探测阶段中，从一点出发，依次沿n个单位正交方向进行探测移动，一轮探测后，再从1个方向开始继续探测，经过若干轮探测移动，完成一个探测阶段。然后构造一组新的单位正交方向，称之为转轴，在下一次迭代中，将沿这些方向进行探测。

1. 探测阶段

首先给定初始点$x_1$，放大因子$\alpha>1$，缩减因子$\beta \in(-1,0)$，还要给定初始搜索方向（一般取坐标方向）和步长，然后进行探测移动。

设第k次迭代的初始点为$x_k$，搜索方向
$$
d_1,d_2,\cdots,d_n
$$
它们是单位正交方向，沿各方向的步长为
$$
\delta_1,\delta_2,\cdots,\delta_n
$$
每轮探测的起点和终点分别用$y_1$和$y_{n+1}$表示。令$y_1 = x_k$。开始第一轮探测移动。

先从$y_1$出发，沿$d_1$探测。
如果$f(y_1+\delta_1d_1)<f(y_1)$，则探测成功，令
$$
y_2 = y_1+\delta_1d_1
$$
并且令$\delta_1 := \alpha \delta_1$，已备下一轮探测时沿$d_1$方向增大步长。

如果$f(y_1+\delta_1d_1)\ge f(y_1)$，则探测失败，令
$$
y_2 = y_1
$$
并且令$\delta_1:=\beta \delta_1$。这样，下一轮探测时，用$\delta_1$乘$d_1$，实际上就是取$d_1$的反向，同时也缩短了步长。

再从$y_2$出发，沿$d_2$做探测移动得到$y_3$。按照这样的方法探测下去，直至沿$d_n$探测得到$y_{n+1}$，完成一轮探测之后，令
$$
y_1 = y_{n+1}
$$
进行下一轮探测，如此一轮一轮地探测下去，直到某一轮沿n个方向的探测均失败，本探测阶段才可能完结。第k次迭代的探测阶段结束时，得到的点记作
$$
x_{k+1} = y_{n+1}
$$
它也是下一次迭代的初始点。

2. 构造新的搜索方向

构造一组新的单位正交方向的方法分作两步，先利用当前的搜索方向和迭代中得到的数据构造一组线性无关的方向，然后利用施密特正交化方法，把他们正交化及单位化。

根据上一阶段探测结果，有
$$
x_{k+1} = x_k + \sum_{i=1}^n\lambda_id_i\tag{11.2.4}
$$
其中$\lambda_i$是在整个探测阶段中所有沿方向$d_i$的步长的代数和。由（11.2.4）式有
$$
x_{k+1}-x_{k} = \sum_{i=1}^n\lambda_id_i
$$

向量$p=x_{k+1}-x_{k}$很有可能是有利于函数值下降的方向。因此新构造的方向组中应包含这个方向，即包含从探测阶段的初点指向该探测阶段的终点的方向。于是定义一组方向$p_1,p_2,\cdots,p_n$如下：

$$
p_j = 
\begin{cases}
d_j \quad , \qquad \lambda_j = 0\\
\sum_{i=j}^n\lambda_id_i\quad , \quad\lambda_j\neq 0
\end{cases}
$$

再利用施密特正交化方法，把向量组$\{p_j\}$正交化，令
$$
q_j = 
\begin{cases}
p_j \quad , \qquad j = 1\\
p_j-\sum_{i=1}^{j-1} \frac{q_i^Tp_j}{q_i^Tq_i}q_i\quad , \quad j\ge 2
\end{cases}
$$

再单位化，令
$$
\bar{d_j} = \frac{q_j}{||q_j||}
$$

由于$d_1,d_2,\cdots,d_n$线性无关且相互正交，因此用上述方法构造的方向集$\bar{d_1},\bar{d_2},\cdots,\bar{d_n}$线性无关，互相正交。

构造n个单位正交方向之后，即可进入下一次迭代的探测阶段。

### 11.2.2 计算步骤 

![](https://files.mdnice.com/user/25190/9413c245-6543-45b6-82ea-9406460aecd8.png)

例题：

![](https://files.mdnice.com/user/25190/b206f505-e83b-4244-85cd-0f1719ff74cf.png)

Risenbriock_Method.m文件如下：

```matlab
%Method of Rosenbriock using line searches

function [xmin,fmin]=Risenbriock_Method(x0)

epsilon=1e-5;
x_k=x0;
k=0;
n=length(x0);
D=eye(n);% 单位阵
Lambda=zeros(n,1);% n*1的零矩阵

while 1
    % orthogonal search
    z_j=x_k;
    for j=1:n
        d_j=D(:,j);    
        lambda_j=fminbnd(@(lambda) theta(lambda,z_j,d_j),-10,10);% 使用精确线搜索确定步长
        Lambda(j)=lambda_j;
        z_j=z_j+lambda_j*d_j;
    end
    x_kplus1=z_j;
    
    % stop criteria
    if (norm(x_kplus1-x_k)<epsilon) && (objective_fun(x_kplus1)-objective_fun(x_k)<epsilon)
        xmin=x_kplus1;
        fmin=objective_fun(xmin);
        k
        return;
    end
    
    % Gram-Schmidt procedure
    D=Gram_Schmidt_Procedure(D,Lambda);% 正交化重新构造正交方向
    
    % iteration
    x_k=x_kplus1;
    k=k+1;
end

        
```

目标函数``objective_fun(x)``如下：
```matlab
function y=objective_fun(x)

y=(x(1)-3)^2+2*(x(2)+2)^2;
```

``Gram_Schmidt_Procedure(D,Lambda)``函数用来完成施密特正交化：
```matlab
function D=Gram_Schmidt_Procedure(D,lambda)

n=length(lambda);
A=zeros(n);
B=zeros(n);
sum=zeros(n,1);

for j=1:n
    if abs(lambda(j))<=0.0001
        A(:,j)=D(:,j);
    else
        for i=j:n
            A(:,j)=A(:,j)+lambda(i)*D(:,i);
        end
    end
end
for j=1:n
    if j==1
        B(:,j)=A(:,j);
        D(:,j)=B(:,j)/norm(B(:,j));
    else
        for i=1:j-1
            sum=sum+(A(:,j)'*D(:,i))*D(:,i);
        end
        B(:,j)=A(:,j)-sum;
        D(:,j)=B(:,j)/norm(B(:,j));
    end
end
```

用于精确线搜索的theta函数为：

```matlab
function y=theta(lambda,x,d)

y=objective_fun(x+lambda*d);
```

最后再命令行窗口输入
```matlab
x0=[0;0];
[xmin,fmin]=Risenbriock_Method(x0)
```

得到结果为：
```matlab
k =
     1

xmin =
    3.0000
   -2.0000

fmin =
   1.9722e-31
```

可见和解析法求得的结果是一致的。

## 11.3 单纯形搜索法

这里介绍的单纯形搜索方法是一种无约束最优化的直接方法，并不是线性规划的单纯形方法。

所谓单纯形是指n维空间$R^n$中具有$n+1$个顶点的凸多面体。比如一维空间中的线段，二维空间中的三角形，三维空间中的四面体等，均为相应空间中的单纯形。

### 11.3.1 基本思想
在给定$R^n$中一个单纯形后，求出$n+1$个顶点上的函数值，确定出有最大函数值的点（称为最高点）和最小函数值的点（称为最低点），然后通过反射，扩展、压缩等方法（几种方法不一定同时使用）求出一个较好的点，用它取代最高点，构成新的单纯形，或者通过向最低点收缩形成新的单纯形，用这样的方法逼近极小点。

算法推导，以二维为例：

![](https://files.mdnice.com/user/25190/4618619c-b95f-4919-9148-a6804bfb8a92.png)

以上几种情形，不论属于哪一种，所得到的新的单纯形，必有一个顶点其函数值小于或等于原单纯形各顶点的上的函数值。每得到一个新的单纯形后，再重复以上步骤，直至满足收敛准则为止。

### 11.3.2 计算步骤

![](https://files.mdnice.com/user/25190/026ac84b-fa70-4497-9c10-2d8c875dcb16.png)
