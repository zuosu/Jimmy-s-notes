# 周报（时间：220805）
本周主要完成的工作有：  
1. 学习了python中的字典、判断、循环、函数。
2. 《李宏毅2021/2022春机器学习课程》看到了41p，学习了自注意力机制，RNN，LSTM。
3. 整理了《最优化理论与算法》第十章中最速下降法、牛顿法、共轭方向法的内容，如下。

# 第十章 无约束非线性规划的导数方法
本章和第11章研究无约束问题最优化方法，无约束问题的算法大致分为两类：
1. 在计算过程中要用到目标函数的导数（本章介绍）
2. 只用到目标函数值（第十一章介绍）

## 10.1 最速下降法
### 10.1.1 最速下降方法
考虑无约束问题
$$
min \quad f(x),\quad x\in R^n
$$
其中函数$f(x)$具有一阶连续偏导数。

最速下降方向：在某点处，使目标函数下降最快的方向。
1. 函数$f(x)$在点$x$处沿方向$d$的变化率可用方向导数来表达。对于可微函数，方向导数等于梯度与方向的内积。即
$$
Df(x;d) = \nabla^Tf(x)d
$$
___
证明：
$$
\begin{aligned}
Df(x;d) &= lim_{\lambda \rightarrow 0^+}\frac{f(x+\lambda d)-f(x)}{\lambda}\\
&=lim_{\lambda \rightarrow 0^+}\frac{f(x)+\lambda \nabla^Tf(x)d+\circ(||d||)-f(x)}{\lambda}\\
&=\nabla^Tf(x)d+lim_{\lambda \rightarrow 0^+}\frac{\circ(||d||)}{\lambda}\\
&=\nabla^Tf(x)d
\end{aligned}
$$
___

2. $f(x)$在点$x$处下降最快的方向，可归结为求解下列非线性规划：
$$
\begin{cases}
min \quad \nabla^Tf(x)d\\
s.t. \quad ||d||\le 1
\end{cases}
$$
___
为什么是求最小 $\nabla^Tf(x)d$，因为下降意味着负值，下降最快所以要求min。
___
根据Schwartz不等式，有
$$
||\nabla^Tf(x)d||\le ||\nabla f(x)||\times||d||\le||\nabla f(x)||
$$
去掉绝对值符号，可以得到
$$
\nabla^Tf(x)d\ge -||\nabla f(x)||
$$
从而知道，当
$$
d = -\frac{\nabla f(x)}{||\nabla f(x)||}
$$
时等号成立。从而得到在点x处沿着其负梯度方向为最速下降方向。

### 10.1.2 最速下降法
#### 基本思想
下降方向使用最速下降方向，步长使用精确线搜索。

#### 计算步骤

![](https://files.mdnice.com/user/25190/2e18f99c-3278-48d7-b46b-3fa1cdc15dfb.png)

例题：

![](https://files.mdnice.com/user/25190/8950b853-c42f-49ba-819d-bd89c660c515.png)

上述题目的最小值点和最小值是显而易见的，最小值点为$[0,0]$ ，最小值为0。

创建最速下降法SteepestDescentDirctionMehtod.m文件

```matlab
function [fstar, xstar] = SteepestDescentDirctionMehtod(x0)
% parameters
epsilon = 1e-4;
xk = x0;

% stop checking
while 1
    gk = objfun_grad(xk);
    Dk =[Dk -gk];
    if norm(gk) <= epsilon
        fstar = objfun(xk);
        xstar = xk;
        return; 
    else
        dk = -gk; % search direction
        alphak = Bisection(xk,dk,0,2); % line search for step size二分法计算步长
        xk = xk+alphak*dk;
    end
end
```

objfun_grad函数对应目标函数的梯度$4*x(1);2*x(2)$故编写objfun_grad.m文件如下

```matlab
function y = objfun_grad(x)

y = [4*x(1);2*x(2)];
```

objfun即为目标函数objfun.m文件如下

```
function y = objfun(x)

y = 2*x(1)^2+x(2)^2;
```

Bisection函数是使用二分法求取步长$\alpha_k$，对应的Bisection.m文件如下

```matlab
function y = Bisection(xk,dk,ak,bk)

% parameters 
epsilon = 1e-1;
eta = 0.001;

% main program
while 1
    if bk-ak <= epsilon
        y = (bk+ak)/2;
        return;
    else
        mid = (bk+ak)/2;
        lambdak = mid-eta;
        muk = mid+eta;
        flambdak = phi(xk,dk,lambdak);
        fmuk = phi(xk,dk,muk);
        if flambdak > fmuk
            ak = lambdak;
        else 
            bk = muk;
        end
    end
end
```

在matlab的命令行输入``[fstar,xstar]=SteepestDescentDirctionMehtod(x0)``可以得到目标函数的最小值和最小值点

```matlab
fstar =

   4.1705e-10


xstar =

   1.0e-04 *

   -0.0530
    0.1900
```

可见结果和预期的一致。
### 10.1.3 最速下降法的收敛性
1. 定理10.1.1 设$f(x)$是连续可微实函数，解集合$\Omega=\{\bar{x}|\nabla f(\bar{x})\}$，最速下降算法产生的序列${x_k}$含于某个紧集，则序列$\{x_k\}$的某个聚点$\hat{x}\in \Omega$。

2. 定理10.1.2 设$f(x)$存在连续二阶偏导数，$\bar{x}$是局部极小点，Hesse矩阵$\nabla^2f(\bar{x})$的最小特征值$a>0$，最大特征值为$A$，算法产生的序列$\{x_k\}$收敛于点$\bar{x}$，则目标函数值的序列${f(x_k)}$以不大于
$$
(\frac{A-a}{A+a})^2
$$
的收敛比线性地收敛于$f(\bar{x})$。

在上述定理中，若令$r=A/a$，则
$$
(\frac{A-a}{A+a})^2=(\frac{r-1}{r+1})^2<1
$$

$r$是对称正定矩阵$\nabla^2f(\bar{x})$的**条件数**。这个定理表明，条件数越小，收敛越快；条件数越大收敛越慢。

3. 最速下降法存在锯齿现象

用最速下降法极小化目标函数时，相邻两个搜索方向是正交的。令
$$
\begin{aligned}
&\varphi(\lambda) = f(x_k+\lambda d_k),\\
&d_k = -\nabla f(x_k)
\end{aligned}
$$
为求出从$x_k$出发沿方向$d_k$的极小点，令
$$
\varphi'(\lambda)=\nabla f(x_k+\lambda d_k)^Td_k=0
$$
又
$$
\begin{aligned}
&d_{k+1} = x_k+\lambda d_k\\
&d_k = -\nabla f(x_k)
\end{aligned}
$$
则
$$
\nabla f(x_k+\lambda d_k)^Td_k=-\nabla f(x_{k+1})^T\nabla f(x_k)=0
$$
即方向$d_{k+1}=-\nabla f(x_{k+1})$与$d_k=-\nabla f(x_k)$正交。表明迭代产生的序列${x_k}$所循路径是“之”字形的。当$x_k$接近极小点$\bar{x}$时，每次移动的步长很小，这样呈现出的锯齿现象，因此影响了收敛速率。

![](https://files.mdnice.com/user/25190/1e3c141f-ad0b-40ac-86df-24cf8628b53f.png)

当Hesse矩阵$\nabla ^2f(\bar{x})$的条件数比较大时，锯齿现象的影响尤为严重。

粗略地讲，在极小点附近，目标函数一般可用二次函数近似,其等值面接近椭球面，长轴和短轴分别位于对应最小特征值和最大特征值的特征向量的方向,，其大小与特征值的平方根成反比。最小特征值与最大特征值相差越大，椭球面越扁，这就使得一维搜索沿着斜长谷进行。因此，当条件数很大时，要使迭代点充分接近极小点,就需要走很大的弯路，计算效率很低。

综上所述，最速下降方向反映了目标函数的一种局部性质。从局部看，最速下降方向确实是函数值下降最快的方向，选择这样的方向进行搜索是有利的。但从全局看，由于锯齿现象的影响，即使向着极小点移近不太大的距离，也要经历不小的弯路，因此使收敛速率大为减慢。最速下降法并不是收敛最快的方法，相反，从全局看，它的收敛是比较慢的。因此，最速下降法一般适用于计算过程的前期迭代或作为间插步骤。当接近极小点时，再使用最速下降法，试图用这种方法达到迭代的终止，这样做并不有利。


## 10.2 牛顿法
### 10.2.1 牛顿法
第九章在一维搜索中的牛顿法，本章加以推广，给出求解一般无约束问题的牛顿法。考虑无约束问题
$$
min \quad f(x),\quad x\in R^n
$$
其中函数$f(x)$具有二阶连续偏导数。

#### 基本思想
在迭代点$x_k$处将目标函数用二次函数近似，用二次函数的极小点近似目标函数的极小点。

![](https://files.mdnice.com/user/25190/c144e13f-3a08-464a-bf23-92b883473673.png)

图中箭头的那一部分是怎么得到的？
___
如果$f(x_k)$是连续的则$f(x_k)$的Hesse矩阵是对称的。
设$f(x)=x^TAx$则$\nabla f(x)=\frac{1}{2}(A^T+A)x$。带入这个公式可以得到图中的结果。
___

####  计算步骤

![](https://files.mdnice.com/user/25190/32d7089d-7486-4a98-a747-60de185510e0.png)



举例：

![](https://files.mdnice.com/user/25190/77a42cf3-be00-4797-bb22-d65b0948ae15.png)

根据题目可以看出最小值点为$(1,0)^T$

```matlab
function [fmin,xmin]=NewtonMethod(x0)

% initialization
epsilon=1.0e-15;
fmin=inf;
xmin=x0;
xk=x0;

% iteration
while 1
   % stop criterion
   gk=grad_obj(xk);
   if norm(gk)<epsilon
       xmin=xk;
       fmin=obj(xmin);
       break;
   else
       % search direction
       Hk=H_obj(xk);
       dk=-inv(Hk)*gk;%inv取逆
       % step size
%       alphak=fminbnd(@(alpha) phi(alpha,xk,dk),0,10);
	   alphak=1;%步长设为1
       % update point
       xk=xk+alphak*dk;       
   end
end
```

grad_obj函数是目标函数的梯度函数，对应grad_obj.m

```matlab
function y=grad_obj(x)

global counter
counter=counter+1;

y=[4*(x(1)-1)^3;2*x(2)];

```

obj为目标函数，对应obj.m

```matlab
function y=obj(x)

y=(x(1)-1)^4+x(2)^2;
```

H_obj为目标函数的hesse矩阵，对应H_obj.m

```matlab
function y=H_obj(x)

y=[12*(x(1)-1)^2 0;0 2];
```

设初始点为x0=[0;1]，在命令行输入
```
x0=[0;1];
[fmin,xmin]=NewtonMethod(x0)
```

得到的结果如下：
```matlab
fmin =

   7.3969e-22


xmin =

    1.0000
         0

```

可见的得到的结果是最小值为0，最小值点是[1;0]，这与预期的结果是一致的。

#### 收敛性
1. 牛顿法至少是2阶收敛

![](https://files.mdnice.com/user/25190/d262b619-e55f-47c0-bd95-f686f9efc348.png)

2. 对于二次函数，用牛顿法求解具有二次**终止性**

![](https://files.mdnice.com/user/25190/1c11ab78-52fe-4467-a58f-094a7f1c7271.png)

3. 值得注意的是当初始点远离极小点时，牛顿法可能不收敛，原因之一，牛顿方向
$$
d=-\nabla ^2f(x)^{-1}\nabla f(x)
$$
不一定是下降方向，经迭代，目标函数值可能上升。此外，即使目标函数值下降，得到的点$x_{k+1}$也不一定是沿牛顿方向的最好点或极小点。

总结牛顿法的缺点：
* 具有局部收敛性，初始点必须非常接近最优解。
* 计算复杂度高（要求Hesse矩阵）
* $\nabla^2f(x_k)$可能是奇异矩阵，其逆矩阵不一定存在

因此人们提出了牛顿法的改进。

### 10.2.2 阻尼牛顿法
#### 基本思想
阻尼牛顿法与原始牛顿法之间的区别在于增加了沿牛顿方向的**一维搜索**（更准确地说，原始牛顿法的步长设为1，而阻尼牛顿法的步长是用一维搜索算出来的），其迭代公式是：

$$
x^{k+1}=x^{k}+\lambda d_n^k
$$

其中$d_n^k=-\nabla ^2f(x_k)^{-1}\nabla f(x_k)$为牛顿方向，$\lambda_k$是一维搜索得到的步长，即满足

$$
f(x_k+\lambda_kd_n^k)=min_\lambda f(x_k+\lambda_kd_n^k)
$$

步长$\lambda_k$可以小于0。

#### 计算步骤
![](https://files.mdnice.com/user/25190/06f3f9a1-5e8d-4010-a27f-f367568a20ab.png)

由于阻尼牛顿法含有一维搜索，因此每次迭代目标函数值一般有所下降（绝不会上升）。可以证明，阻尼牛顿法在适当的条件下具有全局收敛性，且为2级收敛。

### 10.2.3 牛顿法的进一步修正

原始牛顿法和阻尼牛顿法虽然不同，但又共同缺点。
1. 可能出现Hesse矩阵奇异的情形，因此不能确定后继点；
2. 即使$\nabla ^2f(x_k)$非奇异，也未必正定，因而牛顿方向不一定是下降方向，这就可能导致算法失效。

为使牛顿法从任意一点开始均能产生收敛于集合的序列${x_k}$，而要做进一步修正。人们在这一方面做了不少工作，共同的着眼点在于客服Hesse矩阵非正定的困难，下面简介修正牛顿法的一般策略。

1. Glodstein-Price修正牛顿法

![](https://files.mdnice.com/user/25190/d1e6bc3b-b993-435c-a238-fc6a7f15c254.png)

当牛顿方向与负梯度方向的夹角是锐角时，说明牛顿方向是下降方向，此时搜索方向便设为牛顿方向。否则使用负梯度方向。

搜索步长还是使用一维搜索进行计算。

2. Goldfeld 修正牛顿法

使用$G_k$取代hesse矩阵，当Hesse矩阵正定时还是使用Hesse矩阵，否则在Hesse矩阵的基础上加较小正常数$V_k$，使对角元素变大，变为正定。

![](https://files.mdnice.com/user/25190/a72e33fd-ff87-4677-8704-f002bf19e494.png)

## 10.3 共轭方向法
### 10.3.1 共轭方向
#### 定义

![](https://files.mdnice.com/user/25190/5c187243-c2ca-4c6e-8fa4-4154b38d274e.png)

如果$A$为单位阵，则两个方向关于A共轭等价于两个方向正交。
#### 共轭方向的几何意义

![](https://files.mdnice.com/user/25190/2780c5d6-7e64-49da-83f3-c090acfaaf7d.png)

$x_1$是函数$f(x)$等值面上的一点，在点$x_1$处的法向量为$\nabla f(x_1)=A(x_1-\bar{x})$。设$d_1$是在$x_1$处的切向量，$d_2=(\bar{x}-x_1)$，则有$d_1^T\nabla f(x_1)=0$ 即 $d_1^TAd_2=0$。从而可知：
1. 等值面上一点处的切向量与由这一点指向极小点的向量关于$A$共轭；
2. 极小化上图中定义的二次函数，若以此沿着$d_1$和$d_2$进行一维搜索，则经二次迭代必达到极小值。

#### 共轭方向的性质

![](https://files.mdnice.com/user/25190/e853fba2-e4e1-476c-8323-4b90a13aa11b.png)

![](https://files.mdnice.com/user/25190/1c5871f6-c4a4-481c-84fd-37d8af1ebdf6.png)

推论：在扩张子空间定理的条件下，必有
$$
g_{k+1}^Td^{(j)}=0,\quad \forall j\le k
$$

上述定理表明，对于二次凸函数，若沿一组共轭方向（非零向量）搜索，经有限步迭代必达到极小点，这是一种极好的性质，下面将根据这种性质构造一些具有二次终止性的算法。

### 10.3.2 FR共轭梯度法（对于二次函数）

#### 基本思想
把共轭性与最速下降方法相结合，利用已知点处的梯度构造一组共轭方向，并沿这组方向进行搜索，求出目标函数的极小点。根据共轭方向的基本性质，这种方法具有二次终止性。

首先讨论二次凸函数的共轭梯度法，然后把这种方法推广到极小化一般函数。

考虑问题
$$
min \quad f(x)= \frac{1}{2}x^TAx+b^Tx+c \tag{10.3.12}
$$

其中$x\in R^n$，A是正定矩阵，c是常数。

共轭梯度法的图解如下：

![](https://files.mdnice.com/user/25190/5f984bb3-21c4-4b80-a0fd-f9ba4337a103.png)

其中$g_k=\nabla f(x_k),G_k=\nabla^2f(x_k)$，$d_k$和$g_{k+1}$关于A共轭。图中的$\lambda_k$和$\beta_k$如何计算？

![](https://files.mdnice.com/user/25190/e22d287d-2de5-4490-851e-127cde420b9d.png)

定理：

![](https://files.mdnice.com/user/25190/dd666e76-7256-43f4-8cb3-8ad5a3a27abd.png)

上述定理证明Fletcher-Reeves共轭梯度法所产生的搜索方向$d_1,d_2,d_3,\cdots ,d_m$是A共轭的，且经有限步迭代必达极小点。


定理：

![](https://files.mdnice.com/user/25190/eae8047d-16fe-4448-b032-17b6438e2f76.png)

【注】初始搜索方向需要选择最速下降方向（即$d_1=-\nabla f(x_1)$），如果选择别的方向作为初始方向，其余方向均按FR法构造，那么极小化正定时，这样构造出来的一组方向并不能保证共轭性。

#### FR法（对于二次函数）计算步骤

![](https://files.mdnice.com/user/25190/13504c7f-3f6a-42b8-bba9-cb1dedd8097c.png)

例：

![](https://files.mdnice.com/user/25190/8e9505c3-0adc-4221-a92c-f97011e30144.png)

从题目中可以看出最小值为0，最小值点为 [0;0]。

ConjugateGradientMethod.m文件如下：

```matlab
function [fmin,xmin]=ConjugateGradientMethod(x0)

% initialization
xk=x0;
gk=grad_obj(xk);
dk=-gk;

% iteration
for i=1:length(x0)%迭代次数小于等于n
    % line search
    % @是定义句柄的运算符，@(alpha) phi(alpha,xk,dk)意为定义以alpha为变量的函数phi() 
    alphak=fminbnd(@(alpha) phi(alpha,xk,dk),0,10);
    % update xk
    tempgk=gk;
    tempxk=xk;
    tempdk=dk;
    xk=tempxk+alphak*tempdk;
    gk=grad_obj(xk);
    tempbetak=(gk'*(gk-tempgk))/(dk'*(gk-tempgk));%利用定理10.3.4
    dk=-gk+tempbetak*tempdk;
end
xmin=xk;
fmin=obj(xk);
```

grad_obj.m为目标函数的梯度，如下：

```matlab
function y=grad_obj(x)

global counter
counter=counter+1;

y=[2*x(1);4*x(2)];
```

fminbnd - 查找单变量函数在定区间上的最小值。

phi.m如下：
```matlab
function y=phi(alpha,xk,dk)

y=obj(xk+alpha*dk);
```

obj.m为目标函数的文件

```matlab
function y=obj(x)

y=x(1)^2+2*x(2)^2;
```

测试文件test.m如下：

```matlab
global counter
counter=0;

x0=[5;5];

[fmin,xmin]=ConjugateGradientMethod(x0)
counter
```

输出的结果为：
```matlab
fmin =

   4.4758e-28


xmin =

   1.0e-13 *

   -0.1021
   -0.1310


counter =

     3
```

显然，和预期的结果是一致的。

### 10.3.3 用于一般函数的共轭梯度法

#### FR法的推广

上述的FR法针对于二次函数，现把FR法推广到用于极小化任意函数$f(x)$。推广后的共轭梯度法与原来方法的主要区别是：
1. 步长$\lambda_k$不能再用$\lambda_k = -\frac{g_k^Td_k}{d_k^TAd_k}$计算，必须用其他一维搜索方法来确定。
2. 凡用到矩阵A之处，需要用现行点处的Hesse矩阵$\nabla^2f(x^k)$。

显然，用这种方法求任意函数的极小点，一般来说，用有限步迭代是达不到的。迭代的延续可以采取不同的方案。一种是直接延续，即总是用$d_{k+1}=-g_{k+1}+\beta_kd^k$构造搜索方向；一种是**重置**把$n$步作为一轮，每搜索一轮之后，取一次最速下降方向，开始下一轮。

#### FR法（对于任意函数）计算步骤

![](https://files.mdnice.com/user/25190/3467b325-377e-41c3-a53c-44a76b63acb1.png)

在计算因子$\beta_j$，除了使用上面的方法外，还有以下几种形式：

$$
\beta_j = \frac{g_{j+1}^T(g_{j+1}-g_j)}{g_{j+1}^Tg_j}\tag{10.3.25}\\
$$
$$
\beta_j = \frac{g_{j+1}^T(g_{j+1}-g_j)}{d_j^T(g_{j+1}-g_j)} \tag{10.3.26}
$$
$$
\beta_j = \frac{d_j^T\nabla^2f(x_{j+1})g_{j+1}}{d_j^T\nabla^2f(x_{j+1})d_{j}} \tag{10.3.27}
$$

使用（10.3.25）式的共轭梯度法，称为PRP共轭梯度法。

#### 使用共轭梯度法的注意事项

前面的讨论假设采用精确的一维搜索，但是实际计算中，精确一维搜索会带来一定困难，需要付出较大代价，因此许多情形下采用非精确线性搜索，而这样会出现新的问题，按照$d_{k+1}=-g_{k+1}+\beta_kd^k$构造的搜索方向可能不是下降方向。解决这个问题的方法之一，当$d_{k+1}$不是下降方向时，以最速下降方向重新开始，然而，这样做也有问题，当一维搜索比较粗糙时，这样的重新开始可能是大量的，因此会降低计算效率。

还有一种方法，在计算过程中增加附加的检验，设$\bar{g_{k+1}}$，$\bar{d_{k+1}}$，$\bar{\beta_k}$分别表示在检验点处$x_k+\alpha_kd_k$处计算出来的$g_{k+1}，d_{k+1}，\beta_k$，如果满足
$$
-\bar{g_{k+1}}\bar{d_{k+1}}\ge \sigma ||\bar{g_{k+1}}|| \cdot||\bar{d_{k+1}}||
$$

则取$\alpha_k$作为步长$\lambda_k$；否则，进行精确一维搜索，求最优步长$\lambda_k$，这里$\sigma$是一个小的正数。

#### 共轭梯度法的收敛率

共轭梯度法的收敛速率不坏于最速下降法，若使用共轭梯度法而标准初始方向不取$d_1=-\nabla f(x_1)$时，共轭梯度法的收敛速率可能像线性速率那样慢。

共轭梯度法的主要优点是存储量比较小。事实上，FR法只需存储3个n维向量，因此，求解变量多的大规模问题时，可用共轭梯度法。

## 10.4 拟牛顿法

### 10.4.1 拟牛顿条件

牛顿法的收敛很快，但是运用牛顿法需要计算二阶偏导数，而且目标函数的Hesse矩阵可能非正定。为了克服牛顿法的缺点，提出拟牛顿法。它的基本思想是用不包含二阶导数的矩阵近似牛顿法中的Hesse矩阵的逆矩阵。由于构造近似矩阵的方法不同，因而出现不同的拟牛顿法。

怎样构造近似矩阵并用它取代牛顿法中的Hesse矩阵的逆？
___
由10.2节知在点$x_k$处的牛顿方向为
$$d_k=-\nabla ^2f(x_k)^{-1}\nabla f(x_k)$$

设在第k次迭代后，得到点$x_{k+1}$，将目标函数$f(x)$在点$x_{k+1}$展开成Taylor级数，并取二阶近似，得到
$$
\begin{aligned}
f(x)\approx f(x_{k+1})+\nabla f(x_{k+1})^T(x-x_{k+1})\\
+\frac{1}{2}(x-x_{k+1})^T\nabla ^2f(x_{k+1})(x-x_{k+1})
\end{aligned}
$$
对上式两边求梯度，在$x_{k+1}$附近有
$$
\nabla f(x)\approx \nabla f(x_{k+1})+\nabla ^2f(x_{k+1})(x-x_{k+1})
$$
令$x=x_k$，则
$$
\nabla f(x_k)\approx \nabla f(x_{k+1})+\nabla ^2f(x_{k+1})(x_k-x_{k+1})
$$
记
$$
\begin{aligned}
&p_k = x_{k+1}-x_k\\
&q_k = \nabla f(x_{k+1})-\nabla f(x_k)
\end{aligned}
$$
则有
$$
q_k\approx \nabla^2f(x_{k+1})p_k
$$
又设Hesse矩阵$\nabla ^2f(x_k+1)$可逆，则
$$
p_k\approx \nabla^2f(x_{k+1})^{-1}q_k
$$
通过计算$p_k，q_k$可以估计在$x_{k+1}$处的Hesse矩阵的逆。因此为了用不包含二阶导数的矩阵$H_{k+1}$取代牛顿法中的Hesse矩阵$\nabla^2f(x_{k+1})$的逆矩阵，有理由令$H_{k+1}$满足
$$
p_k = H_{k+1}q_k\tag{10.4.7}
$$
将上式称为拟牛顿条件。
____

怎样确定满足这个条件的矩阵$H_{k+1}$。

### 10.4.2 秩1校正

当$\nabla^2f(x_{k+1})^{-1}$是n阶对称正定矩阵时，满足拟牛顿条件的矩阵$H_k$也应是n阶对称正定矩阵。构造这样近似矩阵的一般策略是$H_1$取为任意一个n阶对称正定矩阵，通常选取为n阶单位矩阵$I$，然后通过修正$H_k$给出$H_{k+1}$，令
$$
H_{k+1} = H_k+\Delta H_k
$$
其中$\Delta H_k$称为校正矩阵。
 
确定$\Delta H_k$的方法之一是，令
$$
\Delta H_k = \alpha_k z_kz_k^T
$$
$\alpha_k$是一个常数，$z_k$是n维列向量，这样定义的$\Delta H_k$是秩为1的对称矩阵。$z_k$的选择应使10.4.7式成立。

![](https://files.mdnice.com/user/25190/ae73a5c9-0c19-4ebe-82d9-92f878e157b0.png)

#### 计算步骤

![](https://files.mdnice.com/user/25190/fdc7f5f1-9301-43a0-9c64-c424de18c383.png)

秩1校正，在一定条件下是收敛的，并且具有二次终止性。运用秩1校正也存在一些困难，首先，仅当$q_k^T(p_k-H_kq_k)>0$时，（10.4.13）式得到的$H_{k+1}$才能确保正定性，而这一点是没有保证的，即使$q_k^T(p_k-H_kq_k)>0$由于舍入误差的影响，可能导致$\Delta H_k$无界，从而产生数值计算上的困难，因此这种方法具有局限性。

### 10.4.3 DFP算法

又称为变尺度法，在这种方法中，定义校正矩阵为
$$
\Delta H_k = \frac{p_kp_k^T}{p_k^Tq_k}-\frac{H_kq_kq_k^TH_k}{q_k^TH_kq_k}
$$
得到的$H_{k+1}$为
$$
H_{k+1} = H_k+\frac{p_kp_k^T}{p_k^Tq_k}-\frac{H_kq_kq_k^TH_k}{q_k^TH_kq_k}
$$
经验证，$H_{k+1}$满足拟牛顿条件。

#### 计算步骤

![](https://files.mdnice.com/user/25190/af9b8387-fd34-44e3-9279-6017c2387da9.png)

#### DFP算法的正定性及二次终止性

![](https://files.mdnice.com/user/25190/8f8cdf7a-0192-48fb-a057-aa0d26acb78a.png)









