# 2 感知机
## 2.1 初识感知机
感知机是一个相当重要的机器学习基础模型，神经网络就带有感知机的影子。感知机是一个二分类的线性分类模型，之所以说是线性，是因为它的模型是线性形式的。

### 2.1.1 概念

1. 输入
* 输入空间：$X\subseteq R^n$
* 输入：$x=(x^{(1)},x^{(2)},\cdots x^{(n)})\in X$

2. 输出
* 输出空间：$Y=+1,-1$
* 输出：$y\in Y$
由于这是一个二分类模型，所以这里输出空间是一个只包含+1和-1的一个集合。+1 代表的是正类，-1 代表的是负类，具体的输出则是代表实例$x$所对应的类别。

定义一个从输入空间到输出空间的函数，这个函数就是所谓的感知机。

3. 感知机

$$
f(x)=sign(w \cdot x+b)=
\begin{cases}
+1,\quad w \cdot x+b\ge 0\\
-1,\quad w\cdot x+b\le 0
\end{cases}
$$

其中，$w=(w^{(1)},w^{(2)},\cdots w^{(n)})\in R^n$称为权值（weight），$b\in R$称为偏置（bias），$w\cdot x$表示内积
$$
w\cdot x = w^{(1)}x^{(1)}+w^{(2)}x^{(2)}+\cdots +w^{(n)}x^{(n)}
$$
特征空间里面所有可能的这种线性函数就称为假设空间。

4. 假设空间
* 假设空间$\mathcal F=\{f|f(x)=w\cdot x+b\}$

参数$w$和$b$的所有组合，就得到一个$n+1$维的空间，也就是参数空间
5. 参数空间
参数空间 $\Theta = \{\theta|\theta\in R^{n+1}\}$

### 2.1.2 几何含义

![](https://files.mdnice.com/user/25190/32929a1e-aa63-4bdf-bee9-a281983c7aa9.png)

上述的线性方程
$$
w \cdot x+b
$$
代表着$n$维特征空间$R^n$里面的一个超平面S。

___
超平面的含义：
在几何中，如果环境空间是n维的，那么它所对应的超平面其实就是一个 n-1维的子空间。换句话说，超平面是比他所处的环境空间小一个维度的子空间。

如果特征空间是一维的，想区分正负类实例点，用实数轴上的一个点就可以了，比如零点。
![](https://files.mdnice.com/user/25190/b7599ab0-83c0-4967-a72b-345402c39782.png)
如果特征空间是两维的，实例应该就是二维空间中的一个点，要区分正负类，它的分离超平面对应的应该是一条直线。
![](https://files.mdnice.com/user/25190/ba5eebdd-5916-409c-914d-d63075e6065c.png)

如果可能空间是三维的，分离超平面应该就是一个二维平面了。
那么四维对应的超平面应该就是一个立体面。
依此类推......

___

w是法向量，垂直于超平面S，b是相应的截距项。红色这个箭头，是法向量；绿色这条线，代表的是原点到超平面的距离（如二维中点到直线的距离公式$d = \frac{ax+by+c}{\sqrt{a^2+b^2}}$）。

通过超平面S就可以将整个特征空间分为两部分，一部分是正类，其中的实例所对应的输出为 +1，一部分为负类，它里面的实例所对应的输出为 -1。所以这个超平面被称为分离超平面。

通过前面对感知机模型的介绍，现在可以得到它的流程图

![](https://files.mdnice.com/user/25190/fee0587d-0f91-4550-a51b-d551ac3ce4e4.png)

### 2.1.3 学习策略

以上讲述的是感知的三要素之一，模型。接下来介绍学习策略。

感知机模型要求数据集必须是线性可分的。

___
什么是线性可分？


对于给定的数据集，如果存在某个超平面，使得这个数据集的所有实例点可以完全划分到超平面的两侧，也就是正类和负类。我们就称这个数据集是线性可分的，否则线性不可分。
___

为了寻求一个很好的超平面，就需要制定一定的学习策略（感知机的损失函数）确定模型的参数。

先来看特征空间中任意一点到超平面的距离。

![](https://files.mdnice.com/user/25190/f2d3a62e-8be4-463f-ab04-2cb1c7c8b629.png)

只关注错误分类点，设错误分类点为$x_i$，错误分类点$x_i$到超平面S的距离最终可以用下式来表示
$$
-\frac{1}{||w||}y_i(w \cdot x_i+b)
$$

如果用M代表所有误分类点的集合，我们可以写出所有误分类点到超平面S的距离的总和

$$
-\frac{1}{||w||}\sum_{x_i\in M}y_i(w \cdot x_i+b)
$$

M中所含的误分类点越少，总距离和应越小，在没有误分类点时，M为空集，距离和为0。所以可以通过最小化总距离和来求得相应的参数，所以定义损失函数为
$$
L(w,b) = -\sum_{x_i\in M}y_i(w \cdot x_i+b)
$$

___
与上式相比将w的范数项去除，为什么？
* $||w||$不会影响距离和的符号，即不影响正值还是负值的判断。
* $||w||$不会影响感知器算法的最终结果。算法终止条件，是不存在误分类点。这时候  为空集，那么误分类点的距离和是否为 0 取决于分子，而不是分母，因此与$||w||$的大小无关。
___

## 2.2 感知机的算法
感知机的算法，通常来说有两种，一个是原始形式的，一个是对偶形式的。
在了解算法之前，先来明确要学习的问题：假如给定训练数据集，我们通过最小化损失函数，就可以估计得到模型参数。
![](https://files.mdnice.com/user/25190/2e320a03-79f0-4afd-94fe-64e06eee0a56.png)
### 2.2.1 原始形式

![](https://files.mdnice.com/user/25190/7a04a92c-4a77-463a-908a-101d0bdc2080.png)

需要注意的是，梯度下降法用的是负梯度，所以我们求梯度的时候得到的负号就抵消掉了。

批量梯度下降法需要每次使用所有的误分类点，这会致使每一轮的迭代都需要大量的时间。

随机梯度下降法，每一轮随机选择一个误分类点，迭代的速度会快一些。这是因为，如果通过这个误分类点进行参数的更新，有可能误分类点就会减少，那么下一步我们可用来选择更新参数的实例点就会减少，这在一定程度上简化计算，节约了时间。

小批量梯度下降法既不是像批量梯度下降法中那样选择了所有的样本点，也不是像随机梯度下降法中随机选取了一个样本点，而是选择部分样本点进行参数更新。但是，小批量梯度下降法，也面临许多问题，比如每次需要选择多少个样本点？选择哪些样本才合适呢？这就是模型应用的时候，需要解决的问题了。

接下来以随机梯度下降法来讲解感知机的算法。

![](https://files.mdnice.com/user/25190/4a0f5dff-d8ce-47a3-ae33-f75fa53b7c77.png)

首先选择初始值，假设蓝色的直线对应于初始值代表的分离超平面。

接下来，在训练集中随机选取一个实例点，用 $y_i(w\cdot x_i+b)$ 来判断这个点被分离超平面正确分类还是错误分类。

如果被正确分类， $y_i(w\cdot x_i+b)$ 就是大于零的，我们不用管这个实例点了；如果被错误分类，$y_i(w\cdot x_i+b)$  就是小于零的，我们可以把这个实例点拿来更新参数。

最麻烦的就是，如果这个实例点恰好位于分离超平面上，那么$y_i(w\cdot x_i+b)$   就直接等于零，通过分离超平面把它划分为正类。但是，由于  $y_i(w\cdot x_i+b)$ 等于零，无法知道此时这个实例点到底是被正确分类还是错误分类。所以，本着“宁肯错杀也不能放过”的原则，我们把这个实例点拿来更新参数。

这样第三步就完成了。

接着就是步骤的重复啦，直到所有的实例点都确定被正确分类，咱们的分离超平面就得到了，也就是图中橙色的线。

感知机模型中，w参数  对应于分离超平面的旋转程度， b对应的则是位移量，不停地迭代，就可以使分离超平面越来越接近于能够将所有实例点正确分类的超平面，有可能是黑色的超平面，也有可能是橙色所对应的超平面。

这说明，通过感知机模型所得到的分离超平面是不唯一的。

### 2.2.2 对偶形式
在之前讲解的原始形式的学习算法中，如果实例点 $(x_i,y_i)$是误分类点，可以用它更新参数，即
$$
w\leftarrow w+\eta y_ix_i,b\leftarrow b+\eta y_i
$$

假如，每一个实例点对于参数更新，做了$n_i$次贡献，那么每个实例点作用到初始参数$w_0,b_0$上的增量分别为$\alpha_iy_ix_i$和$\alpha_iy_i$，其中$\alpha_i=n_i\eta_i$。

特别的，如果取初始参数向量为零向量，那么最终学习到的参数就是
$$
w=\sum_{i=1}^N\alpha_iy_ix_i,b = \sum_{i=1}^N\alpha_iy_i
$$

举例解释：
___
![](https://files.mdnice.com/user/25190/f67f6e8e-a38b-4f22-8dba-7f3d3e0ecf44.png)

在这个过程中，实力$(x_1,y_1)$作为误分类点出现两次，所以$n_1=2$，即第一个实例点在迭代中贡献了 2 次。
$(x_2,y_2)$并没有出现，所以$n_2=0$，即第二个实例点在迭代中没有贡献。
$(x_3,y_3)$出现了5次，所以$n_3=5$，即第三个实例点在迭代中贡献了5次。

恰好，$n_1+n_3=7$，就是实际迭代的次数。综合所有贡献的增量，就得到最终的参数了，与原始算法的结果是相同的。
___

对偶形式的基本思想就是通过实例点的线性组合来更新参数，其权重由够昂县的大小决定的。对偶形式的具体步骤如下：

![](https://files.mdnice.com/user/25190/160447cf-86dd-418c-a857-847ea1d65b8b.png)

以上标代表迭代次数，首先选取初始值，比如零向量。然后在训练集中随机选取实例点$(x_i,y_i)$通过
$$
y_i(\sum_{j=1}^N \alpha_jy_jx_j \cdot x_i+b)
$$
判断是否为误分类点，判断标准与原始算法类似，用$y_i(\sum_{j=1}^N \alpha_jy_jx_j \cdot x_i+b)$小于等于零的实例点来更新参数。

之后，重读步骤更新参数，直到训练集中不存在误分类点，停止迭代。最后输出参数的结果即为感知机模型。

对偶形式相比于原始形式好在哪里？
___
观察对偶形式的迭代条件：

![](https://files.mdnice.com/user/25190/8d9ce3cb-ca6a-4303-a713-21ae0590c83c.png)

将迭代条件展开，我们发现，如果训练数据集固定，那么有些值是不需要重复计算的，也就是我们红框里的这N个内积。

如果$(x_i,y_i)$是误分类点，只要读取Gram矩阵第$i$行的值即可。

我们要做的，就是在得到训练数据集之后，把这$N\times N$个内积计算出来储存到Gram矩阵，之后每次更新参数的时候读取就可以，这能节省许多计算量。
___

举例解释：

![](https://files.mdnice.com/user/25190/a673487d-3c28-4baa-a080-04b8be3c519f.png)

![](https://files.mdnice.com/user/25190/60b46d99-cdc7-432c-87cd-a556140d2dd2.png)

先设置初始值，不妨还是取零向量，然后计算Gram矩阵，把9个内积的值都储存下来。

接下来就是判断误分类点，可以选取$(x_1,y_1)$带入迭代条件中，计算得到零，可以用来更新参数，得到一个新的分离超平面。

![](https://files.mdnice.com/user/25190/f408c2d9-69e3-436c-b91a-412aa2140fe4.png)
用这个新得到的分离超平面，对三个实例点分类，其中 $(x_3,y_3)$被错误分类。于是，可以用$(x_3,y_3)$更新参数，又得到一个新的分离超平面。

![](https://files.mdnice.com/user/25190/2491c28a-305a-44e3-a44d-e69c35a9f132.png)

之后，重复步骤，直到没有误分类点，停止迭代。这样，就得到最终模型了。
![](https://files.mdnice.com/user/25190/46d64687-b1e7-4b56-ae79-3fbcd373311b.png)

可以看出，无论是原始形式还是对偶形式，如果迭代的过程是一样的，最后得到的分离超平面还有感知机模型是相同的。

同样的，类似于原始形式的学习算法，对偶形式的学习算法也是收敛的，而且存在多种解。

如果要得到唯一解，需要加约束条件，这就是支持向量机中的内容了。