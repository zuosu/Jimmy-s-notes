# 周报（时间：220916）
本周主要完成的工作有：  
1. 学习了李航《统计学习方法》第二章，第三章、第四章的内容，整理学习笔记如下；
2. 看了19年华为杯D题——汽车行驶工况构建的一篇国二论文。该题是一道数据分析题，论文首先是找出原始数据中不符合题意的异常数据并将其剔除，随后找出运动学片段，计算特征值，使用PCA主成分分析法做降维处理提取出五个主要成分，然后使用K-means聚类算法对运动学片段进行分类，最后通过组合各种类型的运动学片段构建汽车行驶工况，并计算构建出来的汽车工况和采集数据源的特征参数，对其进行比较从而判断所构建结果的优劣。
# 2 感知机
## 2.1 初识感知机
感知机是一个相当重要的机器学习基础模型，神经网络就带有感知机的影子。感知机是一个二分类的线性分类模型，之所以说是线性，是因为它的模型是线性形式的。

### 2.1.1 概念

1. 输入
* 输入空间：$X\subseteq R^n$
* 输入：$x=(x^{(1)},x^{(2)},\cdots x^{(n)})\in X$

2. 输出
* 输出空间：$Y=+1,-1$
* 输出：$y\in Y$
由于这是一个二分类模型，所以这里输出空间是一个只包含+1和-1的一个集合。+1 代表的是正类，-1 代表的是负类，具体的输出则是代表实例$x$所对应的类别。

定义一个从输入空间到输出空间的函数，这个函数就是所谓的感知机。

3. 感知机

$$
f(x)=sign(w \cdot x+b)=
\begin{cases}
+1,\quad w \cdot x+b\ge 0\\
-1,\quad w\cdot x+b\le 0
\end{cases}
$$

其中，$w=(w^{(1)},w^{(2)},\cdots w^{(n)})\in R^n$称为权值（weight），$b\in R$称为偏置（bias），$w\cdot x$表示内积
$$
w\cdot x = w^{(1)}x^{(1)}+w^{(2)}x^{(2)}+\cdots +w^{(n)}x^{(n)}
$$
特征空间里面所有可能的这种线性函数就称为假设空间。

4. 假设空间
* 假设空间$\mathcal F=\{f|f(x)=w\cdot x+b\}$

参数$w$和$b$的所有组合，就得到一个$n+1$维的空间，也就是参数空间
5. 参数空间
参数空间 $\Theta = \{\theta|\theta\in R^{n+1}\}$

### 2.1.2 几何含义

![](https://files.mdnice.com/user/25190/32929a1e-aa63-4bdf-bee9-a281983c7aa9.png)

上述的线性方程
$$
w \cdot x+b
$$
代表着$n$维特征空间$R^n$里面的一个超平面S。

___
超平面的含义：
在几何中，如果环境空间是n维的，那么它所对应的超平面其实就是一个 n-1维的子空间。换句话说，超平面是比他所处的环境空间小一个维度的子空间。

如果特征空间是一维的，想区分正负类实例点，用实数轴上的一个点就可以了，比如零点。
![](https://files.mdnice.com/user/25190/b7599ab0-83c0-4967-a72b-345402c39782.png)
如果特征空间是两维的，实例应该就是二维空间中的一个点，要区分正负类，它的分离超平面对应的应该是一条直线。
![](https://files.mdnice.com/user/25190/ba5eebdd-5916-409c-914d-d63075e6065c.png)

如果可能空间是三维的，分离超平面应该就是一个二维平面了。
那么四维对应的超平面应该就是一个立体面。
依此类推......

___

w是法向量，垂直于超平面S，b是相应的截距项。红色这个箭头，是法向量；绿色这条线，代表的是原点到超平面的距离（如二维中点到直线的距离公式$d = \frac{ax+by+c}{\sqrt{a^2+b^2}}$）。

通过超平面S就可以将整个特征空间分为两部分，一部分是正类，其中的实例所对应的输出为 +1，一部分为负类，它里面的实例所对应的输出为 -1。所以这个超平面被称为分离超平面。

通过前面对感知机模型的介绍，现在可以得到它的流程图

![](https://files.mdnice.com/user/25190/fee0587d-0f91-4550-a51b-d551ac3ce4e4.png)

### 2.1.3 学习策略

以上讲述的是感知的三要素之一，模型。接下来介绍学习策略。

感知机模型要求数据集必须是线性可分的。

___
什么是线性可分？


对于给定的数据集，如果存在某个超平面，使得这个数据集的所有实例点可以完全划分到超平面的两侧，也就是正类和负类。我们就称这个数据集是线性可分的，否则线性不可分。
___

为了寻求一个很好的超平面，就需要制定一定的学习策略（感知机的损失函数）确定模型的参数。

先来看特征空间中任意一点到超平面的距离。

![](https://files.mdnice.com/user/25190/f2d3a62e-8be4-463f-ab04-2cb1c7c8b629.png)

只关注错误分类点，设错误分类点为$x_i$，错误分类点$x_i$到超平面S的距离最终可以用下式来表示
$$
-\frac{1}{||w||}y_i(w \cdot x_i+b)
$$

如果用M代表所有误分类点的集合，我们可以写出所有误分类点到超平面S的距离的总和

$$
-\frac{1}{||w||}\sum_{x_i\in M}y_i(w \cdot x_i+b)
$$

M中所含的误分类点越少，总距离和应越小，在没有误分类点时，M为空集，距离和为0。所以可以通过最小化总距离和来求得相应的参数，所以定义损失函数为
$$
L(w,b) = -\sum_{x_i\in M}y_i(w \cdot x_i+b)
$$

___
与上式相比将w的范数项去除，为什么？
* $||w||$不会影响距离和的符号，即不影响正值还是负值的判断。
* $||w||$不会影响感知器算法的最终结果。算法终止条件，是不存在误分类点。这时候  为空集，那么误分类点的距离和是否为 0 取决于分子，而不是分母，因此与$||w||$的大小无关。
___

## 2.2 感知机的算法
感知机的算法，通常来说有两种，一个是原始形式的，一个是对偶形式的。
在了解算法之前，先来明确要学习的问题：假如给定训练数据集，我们通过最小化损失函数，就可以估计得到模型参数。
![](https://files.mdnice.com/user/25190/2e320a03-79f0-4afd-94fe-64e06eee0a56.png)
### 2.2.1 原始形式

![](https://files.mdnice.com/user/25190/7a04a92c-4a77-463a-908a-101d0bdc2080.png)

需要注意的是，梯度下降法用的是负梯度，所以我们求梯度的时候得到的负号就抵消掉了。

批量梯度下降法需要每次使用所有的误分类点，这会致使每一轮的迭代都需要大量的时间。

随机梯度下降法，每一轮随机选择一个误分类点，迭代的速度会快一些。这是因为，如果通过这个误分类点进行参数的更新，有可能误分类点就会减少，那么下一步我们可用来选择更新参数的实例点就会减少，这在一定程度上简化计算，节约了时间。

小批量梯度下降法既不是像批量梯度下降法中那样选择了所有的样本点，也不是像随机梯度下降法中随机选取了一个样本点，而是选择部分样本点进行参数更新。但是，小批量梯度下降法，也面临许多问题，比如每次需要选择多少个样本点？选择哪些样本才合适呢？这就是模型应用的时候，需要解决的问题了。

接下来以随机梯度下降法来讲解感知机的算法。

![](https://files.mdnice.com/user/25190/4a0f5dff-d8ce-47a3-ae33-f75fa53b7c77.png)

首先选择初始值，假设蓝色的直线对应于初始值代表的分离超平面。

接下来，在训练集中随机选取一个实例点，用 $y_i(w\cdot x_i+b)$ 来判断这个点被分离超平面正确分类还是错误分类。

如果被正确分类， $y_i(w\cdot x_i+b)$ 就是大于零的，我们不用管这个实例点了；如果被错误分类，$y_i(w\cdot x_i+b)$  就是小于零的，我们可以把这个实例点拿来更新参数。

最麻烦的就是，如果这个实例点恰好位于分离超平面上，那么$y_i(w\cdot x_i+b)$   就直接等于零，通过分离超平面把它划分为正类。但是，由于  $y_i(w\cdot x_i+b)$ 等于零，无法知道此时这个实例点到底是被正确分类还是错误分类。所以，本着“宁肯错杀也不能放过”的原则，我们把这个实例点拿来更新参数。

这样第三步就完成了。

接着就是步骤的重复啦，直到所有的实例点都确定被正确分类，咱们的分离超平面就得到了，也就是图中橙色的线。

感知机模型中，w参数  对应于分离超平面的旋转程度， b对应的则是位移量，不停地迭代，就可以使分离超平面越来越接近于能够将所有实例点正确分类的超平面，有可能是黑色的超平面，也有可能是橙色所对应的超平面。

这说明，通过感知机模型所得到的分离超平面是不唯一的。

### 2.2.2 对偶形式
在之前讲解的原始形式的学习算法中，如果实例点 $(x_i,y_i)$是误分类点，可以用它更新参数，即
$$
w\leftarrow w+\eta y_ix_i,b\leftarrow b+\eta y_i
$$

假如，每一个实例点对于参数更新，做了$n_i$次贡献，那么每个实例点作用到初始参数$w_0,b_0$上的增量分别为$\alpha_iy_ix_i$和$\alpha_iy_i$，其中$\alpha_i=n_i\eta_i$。

特别的，如果取初始参数向量为零向量，那么最终学习到的参数就是
$$
w=\sum_{i=1}^N\alpha_iy_ix_i,b = \sum_{i=1}^N\alpha_iy_i
$$

举例解释：
___
![](https://files.mdnice.com/user/25190/f67f6e8e-a38b-4f22-8dba-7f3d3e0ecf44.png)

在这个过程中，实力$(x_1,y_1)$作为误分类点出现两次，所以$n_1=2$，即第一个实例点在迭代中贡献了 2 次。
$(x_2,y_2)$并没有出现，所以$n_2=0$，即第二个实例点在迭代中没有贡献。
$(x_3,y_3)$出现了5次，所以$n_3=5$，即第三个实例点在迭代中贡献了5次。

恰好，$n_1+n_3=7$，就是实际迭代的次数。综合所有贡献的增量，就得到最终的参数了，与原始算法的结果是相同的。
___

对偶形式的基本思想就是通过实例点的线性组合来更新参数，其权重由够昂县的大小决定的。对偶形式的具体步骤如下：

![](https://files.mdnice.com/user/25190/160447cf-86dd-418c-a857-847ea1d65b8b.png)

以上标代表迭代次数，首先选取初始值，比如零向量。然后在训练集中随机选取实例点$(x_i,y_i)$通过
$$
y_i(\sum_{j=1}^N \alpha_jy_jx_j \cdot x_i+b)
$$
判断是否为误分类点，判断标准与原始算法类似，用$y_i(\sum_{j=1}^N \alpha_jy_jx_j \cdot x_i+b)$小于等于零的实例点来更新参数。

之后，重读步骤更新参数，直到训练集中不存在误分类点，停止迭代。最后输出参数的结果即为感知机模型。

对偶形式相比于原始形式好在哪里？
___
观察对偶形式的迭代条件：

![](https://files.mdnice.com/user/25190/8d9ce3cb-ca6a-4303-a713-21ae0590c83c.png)

将迭代条件展开，我们发现，如果训练数据集固定，那么有些值是不需要重复计算的，也就是我们红框里的这N个内积。

如果$(x_i,y_i)$是误分类点，只要读取Gram矩阵第$i$行的值即可。

我们要做的，就是在得到训练数据集之后，把这$N\times N$个内积计算出来储存到Gram矩阵，之后每次更新参数的时候读取就可以，这能节省许多计算量。
___

举例解释：

![](https://files.mdnice.com/user/25190/a673487d-3c28-4baa-a080-04b8be3c519f.png)

![](https://files.mdnice.com/user/25190/60b46d99-cdc7-432c-87cd-a556140d2dd2.png)

先设置初始值，不妨还是取零向量，然后计算Gram矩阵，把9个内积的值都储存下来。

接下来就是判断误分类点，可以选取$(x_1,y_1)$带入迭代条件中，计算得到零，可以用来更新参数，得到一个新的分离超平面。

![](https://files.mdnice.com/user/25190/f408c2d9-69e3-436c-b91a-412aa2140fe4.png)
用这个新得到的分离超平面，对三个实例点分类，其中 $(x_3,y_3)$被错误分类。于是，可以用$(x_3,y_3)$更新参数，又得到一个新的分离超平面。

![](https://files.mdnice.com/user/25190/2491c28a-305a-44e3-a44d-e69c35a9f132.png)

之后，重复步骤，直到没有误分类点，停止迭代。这样，就得到最终模型了。
![](https://files.mdnice.com/user/25190/46d64687-b1e7-4b56-ae79-3fbcd373311b.png)

可以看出，无论是原始形式还是对偶形式，如果迭代的过程是一样的，最后得到的分离超平面还有感知机模型是相同的。

同样的，类似于原始形式的学习算法，对偶形式的学习算法也是收敛的，而且存在多种解。

如果要得到唯一解，需要加约束条件，这就是支持向量机中的内容了。
# 3 K近邻法
## 3.1 K近邻模型
K近邻模型由三个基本要素距离度量、K值的选择和分类决策规则决定。
### 3.1.1 距离度量
特征空间中两个实例点之间的距离是两个实例点相似程度的反应。K近邻模型的特征空间一般是n维实数向量空间$R^n$。使用的距离是欧式距离，但也可以使用其他距离如$L_p$距离和Minkowski距离。

> $L_p$距离的定义：设特征空间$X$是n维实数向量空间$R^n$，$x_i,x_j\in X$，$x_i=(x_i^{(1)},x_i^{(2)},\cdots x_i^{(n)}),x_j=(x_j^{(1)},x_j^{(2)},\cdots x_j^{(n)})$，$x_i,x_j$的$L_p$距离为：
> $$
> L_p(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}
> $$

### 3.1.2 K值的选择
* 减小k值
	* 学习的近似误差会减小
	* 学习的估计误差会增大
	* k值的减小意为着整体模型变得复杂，容易发生过拟合
* 增大k值
	* 学习的估计误差会减小
	* 学习的近似误差会增大
	* k值的增大就意味着整体的模型变得简单
	* 极端的，当k取N时，无论输入实例是什么，都将简单地预测它属于训练实例中最多地类，这时模型过于简单，完全忽略训练实例中地大量有用信息，是不可取的。

在应用中，k值一般取一个比较小的值，**通常采用交叉验证法来选取最优的k值**。

### 3.1.3 分类决策规则
k近邻法的分类决策规则往往是多数表决，即由输入实例的k个临近的训练实例中的多数决定输入实例的类。
## 3.2 k近邻法的实现：kd树
k近邻法的最简单实现方法是线性扫描（linear scan）。这时要计算输入实例与每个训练实例的距离。当训练集很大时，计算非常耗时，这种方法是不可行的。考虑使用特殊的结构来存储训练数据以减少计算距离的次数，kd树就是其中一种特殊结构。
### 3.2.1 构造kd树
构造kd树来储存训练集中的每个实例点

![](https://files.mdnice.com/user/25190/c134a523-20d1-4461-a347-5fd8551e74eb.png)
举例解释：
![](https://files.mdnice.com/user/25190/57e6f21a-1e54-4e17-bf99-f79f1d1f3ccb.png)

在这里选择了$x^{(1)}$作为坐标轴，实际上应选取方差最大的特征作为坐标轴。$x^{(1)}$的数据有八个选取中位数时可以选5或7，这里选择7。7对应的点为$(7,2)$，以此点为切分点，切分整个区域。

![](https://files.mdnice.com/user/25190/83d1d1a4-536d-447f-9c9d-1dc420792628.png)

选择第二个特征作为坐标轴，对于蓝线左面的在第二个特征值上的数有3，4，7取中位数为4，对于蓝线右面的在第二个特征值上取中位数为6

![](https://files.mdnice.com/user/25190/1291ac88-78a1-4f09-9023-7c145ceed3a7.png)

![](https://files.mdnice.com/user/25190/0327b7bd-a608-4f9f-9a53-dd4c1a671fa3.png)

输出kd树

![](https://files.mdnice.com/user/25190/3b959401-25db-47a6-936f-a28a9aff1ded.png)

![](https://files.mdnice.com/user/25190/f5e4859d-ac1a-400a-a816-cf6191d4efdf.png)

### 3.2.2 搜索kd树
最近邻法
怎样搜索kd树？
先从根节点向下搜索当前最近点然后向根节点回溯，回溯过程中发现有更近的点则进行更新。

![](https://files.mdnice.com/user/25190/bf6a86af-1305-478f-9b56-873632a62c6b.png)

![](https://files.mdnice.com/user/25190/f94446d8-5ae5-4ea7-9739-4760dd1c76c7.png)

举例解释：

例题一：给定目标点为$x=(2.1,3.1)$，要求寻找该点的最近邻点。

![](https://files.mdnice.com/user/25190/54142046-37b6-46c9-aeff-67e795a6c38e.png)

![](https://files.mdnice.com/user/25190/5f02c929-259e-4511-9d0e-1b701d57cb0f.png)

$x=(2.1,3.1)$在根节点$(7,2)$的左面，$(5,4)$的左面，在$(2,3)$的右面，至此已经找到最底，因此以叶节点$(2,3)$作为“当前最近点”。

回溯，以目标点x画圆（在二维是圆，多维就是超球面），半径设为当前最近点与目标点之间的距离，画圆的区域与节点$(5,4)$的右面没有交集，与根节点$(7,2)$的右面也没有交集，所以$(2,3)$为“当前最近点”

例题二：目标点$x=(2,4.5)$

![](https://files.mdnice.com/user/25190/14885f97-842b-4d17-8c38-bc33fda61fbc.png)

从根节点向下找，找到当前最近点为$(4,7)$，回溯，以当前最近点与目标点之间的距离作为半径，发现$(4,7)$的父节点在圆区域中，且该圆区域包含了$(4,7)$兄弟节点$(2,3)$的左子区域与右子区域。计算$(2,3)$与目标点之间的距离，小于与$(4,7)$之间的距离，此时对当前最近点进行更新。

![](https://files.mdnice.com/user/25190/f1d157ec-0c3b-4e60-b4c9-2bef929ac730.png)

回溯，以当前最近点$(2,3)$与目标点之间的距离作为半径画圆，发现圆域虽然与父节点$(5,3)$的右子区域有交集但是在该区域内没有其他节点，该区域与根节点的右子区间没有交集，所以将$(2,3)$作为目标点$(2,4.5)$作为最近邻点。

![](https://files.mdnice.com/user/25190/7e5942a0-9774-4224-ad00-faa09c7c5503.png)


# 4 朴素贝叶斯法
## 4.1 朴素贝叶斯的学习与分类
### 4.1.1 贝叶斯定理

![](https://files.mdnice.com/user/25190/f7606edd-2d6e-4198-9952-9b55c1664e70.png)

### 4.1.2 贝叶斯分类

![](https://files.mdnice.com/user/25190/3d0c00e5-9f4b-4140-96fb-7601904ae131.png)

### 4.1.3 朴素贝叶斯

称为“朴素”是因为添加了假设条件：实例特征之间相互独立。

因此
$$
\begin{aligned}
P(X=x|Y=c_i)&=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_i)\\
&=\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_i)
\end{aligned}
$$

![](https://files.mdnice.com/user/25190/2db99877-351d-4ba7-81de-6a668d4fd760.png)

### 4.1.4 朴素贝叶斯分类

![](https://files.mdnice.com/user/25190/783f74a9-d106-42e4-9ff7-0e9224c87268.png)

因为在比较概率$P(Y=c_i|X=x)$是分母是相同的，所以只需比较分子即可，分子越大归属于$c_i$类的概率越大。

### 4.1.5 后验概率最大化的含义
朴素贝叶斯法将实例分到后验概率最大的类中，这就等价于期望风险最小化。假设选择0-1损失函数，期望风险中的期望是对联合分布$P(X,Y)$取的，由此取条件期望
$$
E_x \sum_{i=1}^K[L(c_i,f(X))]P(c_i|X)
$$
$f(X)$是分类决策函数。

![](https://files.mdnice.com/user/25190/0a3b060e-b2a7-41d0-be08-5e8aba04e20d.png)

根据上图右面蓝色的推导可以看出当使期望风险最小化就相当于使后验概率$P(y=c_i|X=x)$最大化。

## 4.2 朴素贝叶斯法的参数估计
在朴素贝叶斯法中，学习意味着估计$P(Y=c_k)$和$P(X^{(j)}=x^{(j)}|Y=c_k)$，而估计的方法有极大似然法和贝叶斯估计。
### 4.2.1 朴素贝叶斯算法（用极大似然法）
问题：给一个实例点$x$，通过朴素贝叶斯的分类法找到它对应的类别$y$。

解决问题的思路：通过后验概率最大化来实现。

![](https://files.mdnice.com/user/25190/deb65d2f-bfe6-4755-92f3-c8c25b63df8a.png)

计算过程：
1. 求出先验概率

![](https://files.mdnice.com/user/25190/2a2b9d34-7c90-4d09-be10-98dfb4e43aa4.png)

2. 求出一系列的条件概率

![](https://files.mdnice.com/user/25190/5d1869a4-592c-4cc1-95d9-656b66ff5945.png)

其中$I$是指示函数。
3. 把各类的先验概率和条件概率连乘，得出计算结果最大值，就是对应的类

![](https://files.mdnice.com/user/25190/deb65d2f-bfe6-4755-92f3-c8c25b63df8a.png)

以上算法中的先验概率和条件概率是应用极大似然估计法估计相应的概率。

### 4.2.2 朴素贝叶斯算法（用贝叶斯估计）
为了避免用极大似然法计算出的概率值为0的情况，贝叶斯估计在先验概率和条件概率的分子和分母上均引入了一个$\lambda$，当$\lambda=0$时就是极大似然估计。

计算过程：
1. 求出先验概率

![](https://files.mdnice.com/user/25190/b2f24f05-a6ea-408b-b088-d5bb23223902.png)

贝叶斯估计常取$\lambda=1$，也称为**拉普拉斯平滑或拉普拉斯估计**。

2. 求出一系列的条件概率

![](https://files.mdnice.com/user/25190/cee2f822-f6b0-4705-80c5-d3609a08fc86.png)

其中$I$是指示函数。

分母中的$S_j$是指第$j$个特征中包含的取值个数。加入参数$\lambda$之后就可以避免因训练集选择不当造成概率为0的情况。

3. 把各类的先验概率和条件概率连乘，得出计算结果最大值，就是对应的类

![](https://files.mdnice.com/user/25190/deb65d2f-bfe6-4755-92f3-c8c25b63df8a.png)