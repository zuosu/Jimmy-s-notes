# 周报（时间：221014）
本周主要完成的工作有： 
1. 学习了逻辑回归的内容；
2. 修改《人工智能》PPT第七章剩余的内容；
# 逻辑回归
逻辑回归是一个分类算法，它可以处理二元分类以及多元分类。虽然它名字里面有“回归”两个字，却**不是一个回归算法**。
## logistic distribution
在研究人口增长时，得出的模型为：
$$
P(t) = \frac{e^{\alpha+\beta t}}{1+e^{\alpha+\beta t}}
$$
将$\alpha+\beta t$替换成$x$得到
$$
F(x) = \frac{e^x}{1+e^x} = \frac{1}{1+e^{-x}}
$$

它的图像如下所示

![](https://files.mdnice.com/user/25190/885636c2-97a2-4cf9-89bf-1ff6a802f06f.png)

观察图像可知$F(x)$满足以下性质
1. $F(x)$单调不减
2. $F(-\infty)=0,F(+\infty)=1$
3. $F(x)$连续所以也是右连续的

即$F(x)$满足分布函数的性质，对$F(x)$进行求导
$$
f(x)=\frac{dF(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2}
$$

它的图像如下

![](https://files.mdnice.com/user/25190/5c59d034-52e7-4eba-9e87-497b5bce39a9.png)

【注】与正态分布的图像很相似，因为它同正态分布、T分布同属于指数分布簇。那平时为什么不使用logistic分布而是使用正态分布呢？因为在给定均值和方差下正态分布是具有最大熵的分布，这样的假设就可以让我们的数据携带信息量最大。

让$F(x)$能够平移$\mu$和改变形状$\gamma$，将$F(x)、f(x)$变为
$$
\begin{aligned}
F(x) = P(X\le x) = \frac{1}{1+e^{-\frac{x-\mu}{\gamma}}}\\
f(x) = F'(x) = \frac{e^{-\frac{x-\mu}{\gamma}}}{\gamma (1+e^{-\frac{x-\mu}{\gamma}})^2}
\end{aligned}
$$
分布函数是一条S型曲线，该曲线也被称为sigmoid曲线，关于点$(\mu,\frac{1}{2})$中心对称。
概率密度函数是一条钟形曲线，中间高两端低，关于$\mu$对称，在此处取得最大值$\frac{1}{4\gamma}$。

## 二元逻辑回归的模型

我们知道，线性回归的模型是求出输出特征向量Y和输入样本矩阵X之间的线性关系系数θ，满足Y=Xθ。此时我们的Y是连续的，所以是回归模型。如果我们想要Y是离散的话，怎么办呢？一个可以想到的办法是，我们对于这个Y再做一次函数转换，变为g(Y)。如果我们令g(Y)的值在某个实数区间的时候是类别A，在另一个实数区间的时候是类别B，以此类推，就得到了一个分类模型。如果结果的类别只有两种，那么就是一个二元分类模型了。逻辑回归的出发点就是从这来的。下面我们开始引入二元逻辑回归。

这个函数g在逻辑回归中我们一般取为sigmoid函数，形式如下：

$$g(z)=\frac{1}{1+e^{-z}}$$

它有一个非常好的性质，即当z趋于正无穷时，g(z)趋于1，而当z趋于负无穷时，g(z)趋于0，这非常适合于我们的分类概率模型。另外，它还有一个很好的导数性质：
$$
g'(z) = g(z)(1-g(z))
$$
这个通过函数对g(z)求导很容易得到，后面我们会用到这个式子。

如果我们令g(z)中的z为：z=xθ，这样就得到了二元逻辑回归模型的一般形式：

$$
h_\theta(x) = \frac{1}{1+e^{-x\theta}}
$$
其中$x$为样本输入，$h_θ(x)$为模型输出，可以理解为某一分类的概率大小。而θ为分类模型的要求出的模型参数。对于模型输出$h_θ(x)$，我们让它和我们的二元样本输出y（假设为0和1）有这样的对应关系，如果$h_θ(x)>0.5$ ，即$xθ>0$, 则y为1。如果$h_θ(x)<0.5$，即$xθ<0$, 则y为0。y=0.5是临界情况，此时xθ=0， 从逻辑回归模型本身无法确定分类。

$h_θ(x)$的值越小，而分类为0的的概率越高，反之，值越大的话分类为1的的概率越高。如果靠近临界点，则分类准确率会下降。

此处我们也可以将模型写成矩阵模式：
$$
h_\theta(X) =  \frac{1}{1+e^{-X\theta}}
$$

其中$h_\theta(X)$为模型的输出，为$m\times 1$的维度。$X$为样本特征矩阵，为$m\times n$的维度。θ为分类的模型系数，为$n\times 1$的向量。

## 二元逻辑回归的损失函数
由于线性回归是连续的，所以可以使用模型误差的的平方和来定义损失函数。但是逻辑回归不是连续的，自然线性回归损失函数定义的经验就用不上了。不过我们可以用最大似然法来推导出我们的损失函数。
假设我们的样本输出是0或者1两类。那么我们有：
$$
\begin{aligned}
&P(y=1|x,θ)=h_θ(x)\\
&P(y=0|x,θ)=1−h_θ(x)
\end{aligned}
$$
把这两个式子写成一个式子，就是：
$$P(y|x,θ)=h_θ(x)^{y}(1−h_θ(x))^{1−y}$$
其中y的取值只能是0或者1。

得到了y的概率分布函数表达式，我们就可以用似然函数最大化来求解我们需要的模型系数θ。

为了方便求解，这里用对数似然函数最大化，对数似然函数取反即为我们的损失函数$J(θ)$。其中：
似然函数的代数表达式为：

$$
L(\theta) = \prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}
$$
其中m为样本的个数。
对似然函数对数化取反的表达式，即损失函数表达式为：
$$
J(\theta)=-\ln L(\theta)=-\sum_{i=1}^{m}\left(y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right)
$$

损失函数用矩阵法表达更加简洁：
$$
J(\theta)=-Y^{T} \operatorname{logh}_{\theta}(X)-(E-Y)^{T} \log \left(E-h_{\theta}(X)\right)
$$
其中E为全1向量$\begin{pmatrix}1\\1\\\vdots\\1\end{pmatrix}$。

## 二元逻辑回归的损失函数的优化方法

对于二元逻辑回归的损失函数极小化，有比较多的方法，最常见的有梯度下降法，坐标轴下降法，牛顿法等。这里推导出梯度下降法中θ每次迭代的公式。由于代数法推导比较的繁琐，这里给出矩阵法推导二元逻辑回归梯度的过程。

对于$J(\theta)=-Y^{T} \operatorname{logh}_{\theta}(X)-(E-Y)^{T} \log \left(E-h_{\theta}(X)\right)
$，求其对 $\theta$ 的偏导得
$$
\frac{\partial}{\partial \theta} J(\theta)=X^{T}\left[\frac{1}{h_{\theta}(X)} \odot h_{\theta}(X) \odot\left(E-h_{\theta}(X)\right) \odot(-Y)\right]+X^{T}\left[\frac{1}{E-h_{\theta}(X)} \odot h_{\theta}(X) \odot\left(E-h_{\theta}(X)\right) \odot(E-Y)\right]
$$
这一步我们用到了向量求导的链式法则，和下面三个基础求导公式的矩阵形式：
$$
\begin{array}{l}
\frac{\partial}{\partial x} \log x=1 / x \\
\frac{\partial}{\partial z} g(z)=g(z)(1-g(z))(g(z) \text { 为 } s i g m o i d \text { 函数 },z=X\theta) \\
\frac{\partial x \theta}{\partial \theta}=x
\end{array}
$$
对于刚才的求导公式我们进行化简可得：
$$
\begin{aligned}
\frac{\partial}{\partial \theta} J(\theta)&=-X^TY[E-h_\theta(X)]+X^Th_\theta(X)(E-Y)\\
&=X^{T}\left(h_{\theta}(X)-Y\right)
\end{aligned}
$$
从而在梯度下降法中每一步向量θ的迭代公式如下：
$$
\theta=\theta-\alpha X^{T}\left(h_{\theta}(X)-Y\right)
$$
其中，$α$为梯度下降法的步长。

## 二元逻辑回归的正则化
逻辑回归也会面临过拟合问题，所以我们也要考虑正则化。常见的有L1正则化和L2正则化。逻辑回归的L1正则化的损失函数表达式如下：
$$
J(\theta)=-Y^{T} \bullet \operatorname{logh}_{\theta}(X)-(E-Y)^{T} \bullet \log \left(E-h_{\theta}(X)\right)+\alpha\|\theta\|_{1}
$$
相比普通的逻辑回归损失函数，增加了L1的范数做作为惩罚，超参数$α$作为惩罚系数，调节惩罚项的大小。其中$||θ||_1$为θ的L1范数。
逻辑回归的L1正则化损失函数的优化方法常用的有**坐标轴下降法**和**最小角回归法**。

二元逻辑回归的L2正则化损失函数表达式如下：
$$
J(\theta)=-Y^{T} \bullet \log h_{\theta}(X)-(E-Y)^{T} \bullet \log \left(E-h_{\theta}(X)\right)+\frac{1}{2} \alpha\|\theta\|_{2}^{2}
$$

逻辑回归的L2正则化损失函数的优化方法和普通的逻辑回归类似。

> L1范数：
> $||\theta||_1 = \sum_{i=1}^m|\theta_i| = |\theta_1|+\cdots+|\theta_m|$
> L2范数：
> $||\theta||_2 = (|\theta_1|^2+\cdots+|\theta_m|^2)^{1/2}$

## 二元逻辑回归的推广(多元逻辑回归)
前面几节的逻辑回归的模型和损失函数都局限于二元逻辑回归，实际上二元逻辑回归的模型和损失函数很容易推广到多元逻辑回归。比如总是认为某种类型为正值，其余为0值，这种方法为最常用的one-vs-rest，简称OvR。
另一种多元逻辑回归的方法是Many-vs-Many(MvM)，它会选择一部分类别的样本和另一部分类别的样本来做逻辑回归二分类。最常用的是One-Vs-One（OvO）。OvO是MvM的特例。每次我们选择两类样本来做二元逻辑回归。
这里只介绍多元逻辑回归的softmax回归的一种特例推导：
在二元逻辑回归中：
$$
\begin{array}{l}
P(y=1 \mid x, \theta)=h_{\theta}(x)=\frac{1}{1+e^{-x} \theta}=\frac{e^{x \theta}}{1+e^{x} \theta} \\
P(y=0 \mid x, \theta)=1-h_{\theta}(x)=\frac{1}{1+e^{x} \theta}
\end{array}
$$

其中y只能取到0和1。则有：
$$
\ln \frac{P(y=1 \mid x, \theta)}{P(y=0 \mid x, \theta)}=x \theta
$$
如果我们要推广到多元逻辑回归，则模型要稍微做下扩展。我们假设是K元分类模型,即样本输出y的取值为1，2，。。。，K。根据二元逻辑回归的经验，我们有：
$$
\begin{array}{l}
\ln \frac{P(y=1 \mid x, \theta)}{P(y=K \mid x, \theta)}=x \theta_{1} \\
\ln \frac{P(y=2 \mid x, \theta)}{P(y=K \mid x, \theta)}=x \theta_{2} \\
\cdots \\
\ln \frac{P(y=K-1 \mid x, \theta)}{P(y=K \mid x, \theta)}=x \theta_{K-1}
\end{array}
$$
上面有K-1个方程。加上概率之和为1的方程如下：
$$
\sum_{i=1}^{K} P(y=i \mid x, \theta)=1
$$
从而得到K个方程，里面有K个逻辑回归的概率分布。解出这个K元一次方程组，得到K元逻辑回归的概率分布如下：
$$
\begin{array}{l}
P(y=k \mid x, \theta)=e^{x \theta k} / 1+\sum_{t=1}^{K-1} e^{x \theta_{t}} \quad \mathrm{k}=1,2, \ldots \mathrm{K}-1 \\
P(y=K \mid x, \theta)=1 / 1+\sum_{t=1}^{K-1} e^{x \theta_{t}}
\end{array}
$$

多元逻辑回归的损失函数推导以及优化方法和二元逻辑回归类似。