# 3 K近邻法
## 3.1 K近邻模型
K近邻模型由三个基本要素距离度量、K值的选择和分类决策规则决定。
### 3.1.1 距离度量
特征空间中两个实例点之间的距离是两个实例点相似程度的反应。K近邻模型的特征空间一般是n维实数向量空间$R^n$。使用的距离是欧式距离，但也可以使用其他距离如$L_p$距离和Minkowski距离。

> $L_p$距离的定义：设特征空间$X$是n维实数向量空间$R^n$，$x_i,x_j\in X$，$x_i=(x_i^{(1)},x_i^{(2)},\cdots x_i^{(n)}),x_j=(x_j^{(1)},x_j^{(2)},\cdots x_j^{(n)})$，$x_i,x_j$的$L_p$距离为：
> $$
> L_p(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}
> $$

### 3.1.2 K值的选择
* 减小k值
	* 学习的近似误差会减小
	* 学习的估计误差会增大
	* k值的减小意为着整体模型变得复杂，容易发生过拟合
* 增大k值
	* 学习的估计误差会减小
	* 学习的近似误差会增大
	* k值的增大就意味着整体的模型变得简单
	* 极端的，当k取N时，无论输入实例是什么，都将简单地预测它属于训练实例中最多地类，这时模型过于简单，完全忽略训练实例中地大量有用信息，是不可取的。

在应用中，k值一般取一个比较小的值，**通常采用交叉验证法来选取最优的k值**。

### 3.1.3 分类决策规则
k近邻法的分类决策规则往往是多数表决，即由输入实例的k个临近的训练实例中的多数决定输入实例的类。
## 3.2 k近邻法的实现：kd树
k近邻法的最简单实现方法是线性扫描（linear scan）。这时要计算输入实例与每个训练实例的距离。当训练集很大时，计算非常耗时，这种方法是不可行的。考虑使用特殊的结构来存储训练数据以减少计算距离的次数，kd树就是其中一种特殊结构。
### 3.2.1 构造kd树
构造kd树来储存训练集中的每个实例点

![](https://files.mdnice.com/user/25190/c134a523-20d1-4461-a347-5fd8551e74eb.png)
举例解释：
![](https://files.mdnice.com/user/25190/57e6f21a-1e54-4e17-bf99-f79f1d1f3ccb.png)

在这里选择了$x^{(1)}$作为坐标轴，实际上应选取方差最大的特征作为坐标轴。$x^{(1)}$的数据有八个选取中位数时可以选5或7，这里选择7。7对应的点为$(7,2)$，以此点为切分点，切分整个区域。

![](https://files.mdnice.com/user/25190/83d1d1a4-536d-447f-9c9d-1dc420792628.png)

选择第二个特征作为坐标轴，对于蓝线左面的在第二个特征值上的数有3，4，7取中位数为4，对于蓝线右面的在第二个特征值上取中位数为6

![](https://files.mdnice.com/user/25190/1291ac88-78a1-4f09-9023-7c145ceed3a7.png)

![](https://files.mdnice.com/user/25190/0327b7bd-a608-4f9f-9a53-dd4c1a671fa3.png)

输出kd树

![](https://files.mdnice.com/user/25190/3b959401-25db-47a6-936f-a28a9aff1ded.png)

![](https://files.mdnice.com/user/25190/f5e4859d-ac1a-400a-a816-cf6191d4efdf.png)

### 3.2.2 搜索kd树
最近邻法
怎样搜索kd树？
先从根节点向下搜索当前最近点然后向根节点回溯，回溯过程中发现有更近的点则进行更新。

![](https://files.mdnice.com/user/25190/bf6a86af-1305-478f-9b56-873632a62c6b.png)

![](https://files.mdnice.com/user/25190/f94446d8-5ae5-4ea7-9739-4760dd1c76c7.png)

举例解释：

例题一：给定目标点为$x=(2.1,3.1)$，要求寻找该点的最近邻点。

![](https://files.mdnice.com/user/25190/54142046-37b6-46c9-aeff-67e795a6c38e.png)

![](https://files.mdnice.com/user/25190/5f02c929-259e-4511-9d0e-1b701d57cb0f.png)

$x=(2.1,3.1)$在根节点$(7,2)$的左面，$(5,4)$的左面，在$(2,3)$的右面，至此已经找到最底，因此以叶节点$(2,3)$作为“当前最近点”。

回溯，以目标点x画圆（在二维是圆，多维就是超球面），半径设为当前最近点与目标点之间的距离，画圆的区域与节点$(5,4)$的右面没有交集，与根节点$(7,2)$的右面也没有交集，所以$(2,3)$为“当前最近点”

例题二：目标点$x=(2,4.5)$

![](https://files.mdnice.com/user/25190/14885f97-842b-4d17-8c38-bc33fda61fbc.png)

从根节点向下找，找到当前最近点为$(4,7)$，回溯，以当前最近点与目标点之间的距离作为半径，发现$(4,7)$的父节点在圆区域中，且该圆区域包含了$(4,7)$兄弟节点$(2,3)$的左子区域与右子区域。计算$(2,3)$与目标点之间的距离，小于与$(4,7)$之间的距离，此时对当前最近点进行更新。

![](https://files.mdnice.com/user/25190/f1d157ec-0c3b-4e60-b4c9-2bef929ac730.png)

回溯，以当前最近点$(2,3)$与目标点之间的距离作为半径画圆，发现圆域虽然与父节点$(5,3)$的右子区域有交集但是在该区域内没有其他节点，该区域与根节点的右子区间没有交集，所以将$(2,3)$作为目标点$(2,4.5)$作为最近邻点。

![](https://files.mdnice.com/user/25190/7e5942a0-9774-4224-ad00-faa09c7c5503.png)

