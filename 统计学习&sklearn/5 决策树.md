# 周报（时间：220923）
本周主要完成的工作有：  
1. 学习了李航《统计学习方法》第五章决策树的内容，整理学习笔记如下；
2. 修改《人工智能》PPT第七章RNN的内容
# 5 决策树
决策树（decision tree）是一种基本的分类与回归方法。
决策树呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要有点是模型具有可读性，分类速度快。
* 学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。
* 预测时，对新的数据，利用决策树模型进行分类。

决策树学习通常包括3个步骤：
1. 特征选择
2. 决策树的生成
3. 决策树的修剪
## 5.1 决策树模型与学习
### 5.1.1 决策树模型
决策树定义：决策树由结点（node）和有向边（directed edge）组成。节点有两个类型：内部节点和叶节点。内部节点表示一个特征或属性，叶节点表示一个类。

举例：贷款申请的决策树，分了三个特征：年收入、房产、婚姻状况。

已知：训练集
$$
T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}
$$
其中$x_i=(x_i^{(1)},x_i^{(2)},x_i^{(3)}),y_i\in\{1,2,\cdots,K\},i=1,2,\cdots,N$
目的：构造决策树，并对实例进行分类

这里的三个特征可以表示为$x_i^{(1)}=年收入,x_i^{(2)}=房产,x_i^{(3)}=婚姻状况$，类别为$y_1$=可偿还，$y_2$=无法偿还。

![](https://files.mdnice.com/user/25190/91c8ed55-78fa-4041-bf8b-4695937038a3.png)

这里的椭圆是叶节点，长方形表示内部节点，有向边就是那些箭头。

### 5.1.2 构建决策树

构建决策树的步骤如下：
1. 构建根节点
训练数据集，假如包含N个样本，我们将其置于根结点处，故事就是从这里开始的，也就是说我们的决策树就是**从根结点开始分叉的**。
2. 选择最优特征
在这么多特征里面，选择最优特征来分割训练数据集。
3. 判断是否产生了叶节点
若子集被基本正确分类，**构建叶节点**，否则，收纳更新选择新的最优特征
4. 重复步骤2和步骤3
直到训练数据集中所有样本基本正确分类，**注意：是基本正确分类，原因是为了平衡拟合和泛化能力**

### 5.1.3 决策树与条件概率分布

决策树的根本其实还是一个条件概率模型。
一颗决策树，可以表示成给定特征条件下类的条件概率分布，记作：
$$
P(Y|X)
$$
其中X代表了给定的特征，Y代表了给定特征时所对应的类。
如：
$$
P(Y=+1|X=c)>0.5
$$
表示的含义是给定特征$X=c$时，$Y=+1$的条件概率，即当给定某一特征$X=c$时，如果属于$Y=+1$的概率大于0.5，那么就判定属于$Y=+1$类。

## 5.2 决策树学习之选择最优特征

选择最优特征可以使构建的决策树更加简单。那么改如何选择最优特征呢？
### 5.2.1 选择**信息增益最大的特征作为最优特征**


>什么是信息增益？

信息增益是由熵构建而成，熵起源于热力学，后来由香农引用到信息论中，表示的是**随机变量的不确定性**，不确定性越大，代表着熵越大。

由于熵和随机变量的分布有关，可以写成:
$$
H(p)=-\sum_{i=1}^np_i log \ p_i
$$

什么时候的熵最大？
特征的所有取值概率相同时，所包含的信息是最多的，此时就是不确定性最大的情况。

**计算经验熵公式**
$$
H(D)=-\sum_{k=1}^K\frac{C_k}{D}log_2\frac{C_k}{D}
$$
$D$是样本个数，$C_k$是类别，下同。
**计算经验条件熵公式**
$$
H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log_2\frac{D_{ik}}{D_i}
$$

这里的经验熵和经验条件熵都是根据已有数据估计得到的。在特征A下每个子集所占的权重为
$$
w_i=\frac{D_i}{D}
$$
所以经验条件熵可以表示为一个加权和：
$$
H(D|A)=\sum_{i=1}^nw_iH(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log_2\frac{D_{ik}}{D_i}
$$
**计算信息增益公式**
$$
g(D,A)=H(D)-H(D|A)
$$
即信息增益就是经验熵和经验条件熵的插值，它代表的含义是：**得知特征A而使类Y的信息不确定性减少的程度**。

可以看出经验条件熵越小说明对应的不确定性最小，意味着如果选择特征A为最优特征时，对于分的类是最为确定的，那么我们对应的就希望这个信息增益是最大的。

如果不同特征内的分类个数不同，有的是3个，比如青年、中年、老年，有的则是2个，比如有房子和没房子，取值个数较多时，有可能计算出的信息增益会更大，可以看出，**信息增益会更倾向于取值较多的特征。**

降低取值较多的影响是使用**信息增益比**。

### 5.2.2 选择**信息增益比最大的特征值作为最优特征值**

> 什么是信息增益比？

信息增益比是为了**得到某一个特征单位取值数下的信息增益**。训练数据D关于特征A的熵：
$$
H_A(D)=\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}
$$
信息增益比：
$$
g_R(D,A)=\frac{g(D,A)}{H_A(D)}
$$

3. 哪个指标更好呢

要和实际情况相结合，看看哪种分类下的树形结构更优化。

## 5.3 决策树的生成
### 5.3.1 ID3算法
输入：训练数据集D，特征集A，阈值$\epsilon$
输出：决策树T

* 步骤1：判断T是否需要选择特征生成决策树
	
	* 情况1：若D中所有实例属于**同一类**，则 T为**单结点树**，记录实例类别$C_k$，以此作为该结点的类标记，并返回T。
	
	* 情况2：若D中所有实例**无任何特征**（$A=\emptyset$）,则T为**单节点树**，记录D中实例个数最多类别$C_k$，根据多数表决原则，依次作为该节点的类标记，并返回T。

* 步骤2：否则计算A中各特征的信息增益，并选择**信息增益最大**的特征$A_g$
	* 情况1：若$A_g$的**信息增益小于$\epsilon$**，则T为单结点树，记录D中实例个数最多类别$C_k$，以此作为该结点的类标记，并返回T

	* 情况2：否则，按照$A_g$的每个可能值$a_i$，将D分为若干非空子集$D_i$，将$D_i$中实例个数最多的类别作为标记，构建子结点，以节点和其子节点构成T，并返回T。
* 第$i$个子结点，以$D_i$为训练集，$A-A_g$为特征集合，递归调用以上步骤得到子树$T_i$。

![](https://files.mdnice.com/user/25190/e7cf9fdb-538e-4455-8d69-cbc7a2d8c97d.png)

### 5.3.2 C4.5算法
C4.5算法与ID3相似，C4.5算法对ID3算法进行了改进。C4.5在生成的过程中用信息增益比来选择特征。

## 5.4 决策树的剪枝
和种植一棵真正的树类似，在决策树里对于不需要的枝杈也会进行剪枝。理想决策树是既能对已知数据有良好的**拟合效果**，还能对未知数据有良好的**泛化效果**。剪枝的目标是为了**解决过拟合**的问题。

模型的复杂度体现在两方面：
1. 深度大小
深度是指所有结点的层次数，一般从上往下、从0开始，下面这个例子就可以看出，**深度分别是第0层、第1层和第2层**。
2. 叶节点数量
叶结点就是咱们生成决策树后的最优分类结果，从下面这个图上看出就是绿色框框出的啦，一共有**「3个叶结点」**。

![](https://files.mdnice.com/user/25190/ea45ed0a-6d56-4b35-91d5-8c45b49d73b6.png)

效果又好又简单的模型，就是以上两者的结合。

### 5.4.1 剪枝的分类
1. 预剪枝

生成过程中，对每个结点划分前进行估计，若当前结点的划分不能提升**泛化能力**，则停止划分，记当前结点为叶结点。

2. 后剪枝

生成一棵完整的决策树后，从底部向上对内部结点进行考察，如果将**内部结点变成叶结点**，可以提升泛化能力，那么就进行交换。

### 5.4.2 预剪枝的方法
1. 限定决策树的深度

把多余的层直接去掉，按照多数的原则将内部结点替换为叶节点。

2. 设定一个阈值

在ID3和C4.5算法中阈值出现在信息增益的比较环节，阈值的把不同，生成的树也会不同。

3. 设置某个指标（如误差率、准确率等）比较结点划分前后的泛化能力
>测试集上的误差率：测试集中**错误**分类的实例占比。
>测试集上的准确率：测试集中**正确**分类的实例占比。

测试集上的误差率越高，说明泛化能力越弱；反之，说明泛化能力越强。

### 5.4.3 后剪枝
1. 降低错误剪枝REP

**原理**：自上而下，使用**测试集**剪枝，对每个结点计算剪枝前后的误判个数，如果剪枝满足**减少误判或者相同误判**，则减掉该结点所在分枝。

这个方法和预剪枝中用**测试集误差率来判断是否需要剪枝**的方法类似，不同之处在于后剪枝的方向是**自下而上**，此外在这里使用的是误差个数而不是误差率。

> 降低错误剪枝REP的特点
 -   计算复杂性是线性的，每个结点只需要访问1次
    
-   操作简单，容易理解
    
-   受测试集影响大，体现出了欠拟合的不足，如果测试集比训练集小，就会限制分类的精度

2. 悲观错误剪枝PEP

**原理**：根据剪枝前后的错误率来决定是否剪枝，和REP不同，PEP**只需要训练集即可**，不需要测试集，并且是**从上而下**的剪枝。

> PEP的步骤

1. 计算剪枝前目标子树每个叶子结点的误差，并进行连续修正

$$
\operatorname{Error}\left(\operatorname{Leaf}_i\right)=\frac{\operatorname{error}\left(L e a f_i\right)+0.5}{N(T)}
$$
对于一棵目标子树，先看看有多少叶子结点，然后根据这个式子计算**每个叶子结点的误差率**。

$\operatorname{error}\left(L e a f_i\right)$表示叶子结点$Le a f_i$处误判的样本个数， 表示的是总样本个数，这样就计算出了某一个叶子结点对应的误差率，但是这样计算出的结果是离散的，为了进行连续的修正，在分子上又加上了0.5。

2. 计算剪枝前目标子树的修正误差

$$
\operatorname{Error}(T)=\sum_{i=1}^L \operatorname{Error}\left(\operatorname{Leaf}_i\right)=\frac{\sum_{i=1}^L \operatorname{error}\left(\operatorname{Leaf}_i\right)+0.5 * L}{N(T)}
$$

接着求和就得出了剪枝前的目标子树的修正误差率，L表示的是叶子结点的个数。

3. 计算剪枝前目标子树的修正误差
$$
E(T)=N(T) \times \operatorname{Error}(T)=\sum_{i=1}^L \operatorname{error}\left(L_{e a f_i}\right)+0.5 * L
$$
4. 计算剪枝前目标子树误判个数的标准差
$$
\operatorname{std}(T)=\sqrt{N(T) \times \operatorname{Error}(T) \times(1-\operatorname{Error}(T))}
$$
【注】这里是利用二项分布计算所得标准差
5. 计算剪枝前误判上限（即悲观误差）
$$
E(T)+s t d(T)
$$

6. 计算剪枝后改结点的修正误差
$$
\operatorname{Error}(\operatorname{Leaf})=\frac{\operatorname{error}(\operatorname{Leaf})+0.5}{N(T)}
$$
7. 计算剪枝后该结点误判个数的期望值
$$
E(\text { Leaf })=\operatorname{error}(\text { Leaf })+0.5
$$
8. 比较剪枝前后的误判个数，如果满足以下不等式，则剪枝；否则，不剪枝
$$
E(\text { Leaf })<E(T)+\operatorname{std}(T)
$$

> 悲观错误剪枝的特点
-   不需要分离剪枝数据集，有利于实例少的问题
-   误差使用了连续修正值，适用性更强
-   由于自上而下的剪枝策略，PEP效果更高，但是也可能修剪掉不应修剪的。
## 5.5 CART算法
CART算法展开就是**（Classification and Regression Tree）**，对应的就是分类与回归树，在这里就是用树形结构来解决分类和回归的问题。
-   如果输出变量是**离散**的，对应的就是**分类**问题。
-   如果输出变量是**连续**的，对应的就是**回归**问题。

在CART算法中，树形结构是二叉树模型，使用**基尼指数**来选择最优特征。

假设有K个类，样本点属于第 k类的概率为$p_k$，概率分布的基尼指数定义为：
$$
Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum p^2_k
$$
显然，这就是样本点**被错分的概率期望**。如果整个样本集只有一个类别，那么基尼指数就是0，表示样本集纯度达到最高值。

### 5.5.1 CART分类树算法
输入：数据集D，特征集A，停止条件阈值$\epsilon$
输出：CART分类决策树

1. 从根节点出发，构建二叉树
2. 计算现有特征下对数据集D基尼指数，选择最优特征

	假设特征集A中有$A_1,A_2,\cdots,A_n$个特征，先选出$A_1$特征，假设这个特征里，有$a_{11},a_{12},\cdots a_{1m}$个值，那么对数据集 D按照每一个$a_{1i}$特征值来分成$D_1$和$D_2$两个数据集，并且计算一下对应的基尼指数，选择基尼指数最小的那个特征值$a_{1i}$作为**最优切分点**。以此类推，得出每个特征下的最优切分点，也就是最优的特征值。接着比较在**「最优切分下」**每个特征的基尼指数，选择**基尼指数最小的那个特征**，就是最优特征。
3. 根据最优特征和最优切分点，生成两个子节点，并将数据集分配到对应的子节点中。
4. 分别对两个子节点继续递归调用上面的步骤，直到满足条件，即生成CART分类决策树。

	这里的条件，一般就是阈值$\epsilon$，当基尼指数小于这个阈值时，意味着样本基本上属于一类，或者就是没有更多的特征了，那么就完成了CART分类决策树的生成。

### 5.5.2 CART算法剪枝方法
>损失函数


**原理**：根据剪枝前后的**损失函数**来决定是否剪枝，剪枝后，如果损失函数减小，则意味着可以剪枝。

损失函数为：
$$
C_\alpha=C(T)+\alpha|T|
$$
第一部分$C(T)$反映的是代价，是对训练数据的预测误差（比如基尼指数），也就是模型的**拟合能力**。
第二部分$|T|$反映的是模型的复杂度，体现的是**泛化能力**，$|T|$表示书上叶子结点的个数，叶子结点越多，模型越复杂。
$\alpha$是一个决定拟合和泛化综合效果的参数
-   当$\alpha=0$时，模型仅由拟合决定，不考虑对未知数据的预测能力，所以这样得到是一棵最完整的决策树，泛化能力弱。
    
-   当$\alpha=+\infty$时，得到的是单结点树，对于任何数据的泛化能力很强，但拟合效果差。

可见$\alpha$的取值尤为重要，那么$alpha$该如何取呢？
___
$\alpha$的取值范围是$[0,+\infty)$，每一个$\alpha$对应着一颗决策树，把$\alpha$按照左闭右开的形式划分成小区间。比如：
$$
[\alpha_1,\alpha_2),[\alpha_2,\alpha_3),
\cdots
[\alpha_n,\alpha_n+1)
$$
共有n个区间，每个区间对应着一个决策树记为：
$$
T_0,T_1,T_2,\cdots T_n
$$
这里$T_0$就代表了$\alpha=0$的完整的决策树，意味着没有剪枝。紧接着就是在这些决策树里找到最优的决策树。

**假设现在我们有这么一棵子树，叫做$T_t$**
剪枝前的损失函数可以写成
$$
C_\alpha(T_t)=C(T_t)+\alpha|T_t|
$$
剪枝后变成了一个叶子结点，也就意味着$|T|=1$，那么损失函数可以写成
$$
C_\alpha(T_t)=C(t)+\alpha
$$
假设$\alpha$从0开始逐渐变大到$+\infty$，意味着从高度拟合到高度泛化的变化趋势，可以得出在高度拟合和高度泛化时，所对应的损失函数都是非常大的。而在这变化过程中，意味着剪枝前和剪枝后的会有一个临界值$\alpha$，在这个临界值处，**拟合**和**泛化**的损失函数为最小。
这个值可以联立上面的两个方程求出
$$
\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}
$$
___
> 剪枝算法


输入：CART算法生成的完整决策树
输出：最优决策树$T_\alpha$

1. 设$k=0,T=T_0$
k代表的是迭代的次数，这里从0开始，也就意味着还没开始迭代，那么树也是完整的，从这里开始出发。
2. 设$\alpha=+\infty$
 因为后面要比较大小，当损失函数小的时候可以剪枝。
 3. 自下而上的对个内部结点t计算$C(T_t),|T_t|$，以及
$$
g(t)=\frac{C(t)-C(T_t)}{|T_t|-1},\alpha=min(\alpha,g(t))
$$
	
-    $g(t)$代表了在这个结点对应的$\alpha$值
-    $C(t)$代表了单结点时的预测误差。
-    $C(T_t)$代表了子树时的预测误差。

【注】此处的预测误差还可以包括平方损失、基尼指数等。

4. 自上而下访问内部结点t，如果有$g(t)=\alpha$ ,则进行剪枝，并对叶结点 t以多数表决法决定类，得到树T。
5. 接着增加迭代次数，继续调用，最终采用交叉验证法在子树$T_0,T_1,T_2,\cdots,T_n$中选取最优子树$T_\alpha$

