# 周报（时间：220930）
本周主要完成的工作有：  
1. 学习了决策树在sklearn中的实现，完成了泰坦尼克号幸存者预测的案例，学习了随机森林在sklearn中的实现；
2. 修改《人工智能》PPT第七章剩余的内容

# 泰坦尼克号幸存者预测的案例

```python
from cProfile import label
from symbol import parameters
from turtle import color
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

# 也可以使用pd.read_csv(r"D:\Projects\Python\taitannike\data.csv");
# 如果去掉括号内的r需要写成pd.read_csv("D:/Projects/Python/taitannike/data.csv");
data = pd.read_csv("./data.csv");
data.info()

# 3. 对数据集进行预处理
# 删除缺失值过多的列，和观察判断来说和预测的y没有关系的列
data.drop(["Cabin","Name","Ticket"],inplace=True,axis=1)

#处理缺失值，对缺失值较多的列进行填补，有一些特征只确实一两个值，可以采取直接删除记录的方法
data["Age"] = data["Age"].fillna(data["Age"].mean())
data = data.dropna()# 这种删除是删掉所有缺失值的行，此时缺失的数据只有两个Embarked的数据

#将分类变量转换为数值型变量
labels = data["Embarked"].unique().tolist()# 查看labels取哪几种值并转换为列表
data["Embarked"]=data["Embarked"].apply(lambda x: labels.index(x))# 将Embarked的取值转换为labels的索引

#将二分类变量转换为数值型变量
#astype能够将一个pandas对象转换为某种类型，和apply(int(x))不同，astype可以将文本类转换为数字，用这
#个方式可以很便捷地将二分类特征转换为0~1
data["Sex"] = (data["Sex"]=="male").astype("int")# (data["Sex"]=="male")返回布尔值使用astype("int")转换为数值

#查看处理后的数据集
data.head()

# 4. 提取标签和特征矩阵，分测试集和训练集
X = data.iloc[:,data.columns != "Survived"]
y = data.iloc[:,data.columns == "Survived"]

Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.3)

# 修正测试集和训练集的索引
for i in [Xtrain,Xtest,Ytrain,Ytest]:
    i.index = range(i.shape[0])
Xtrain.head()

#5. 导入模型粗略的查看结果
clf = DecisionTreeClassifier(random_state=25)
clf = clf.fit(Xtrain,Ytrain)
score = clf.score(Xtest,Ytest)
print("score:",score)

## 使用交叉验证方法查看预测结果
score = cross_val_score(clf,X,y,cv=10).mean()
print("cross val score:",score)

## 在不同max_depth下观察曲线的拟合情况
tr = []
te = []
for i in range(10):
    clf = DecisionTreeClassifier(random_state=25
                                ,max_depth=i+1
                                ,criterion="entropy")
    clf = clf.fit(Xtrain,Ytrain)
    score_tr = clf.score(Xtrain,Ytrain)
    score_te = cross_val_score(clf,X,y,cv=10).mean()
    tr.append(score_tr)
    te.append(score_te)
plt.plot(range(1,11),tr,color="red",label="train")
plt.plot(range(1,11),te,color="blue",label="test")
plt.xticks(range(1,11))# 添加横轴的数值
plt.legend()
plt.show()
print(max(te))

# 用网格搜索调整参数（输入参数的取值范围然后去遍历，训练耗时）
## - gini_threholds = np.linspace(0,0.5,10)#在0-0.5之间生成有序的十个数，因为gini指数在0-0.5之间，若是entropy的话，entropy的取值在0-1之间
## - min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生
parameters = {'splitter':('best','random')
                ,'criterion':('gini','entropy')
                ,'max_depth':[*range(1,10)]
                ,'min_samples_leaf':[*range(1,50,5)]
                ,'min_impurity_decrease':[*np.linspace(0,0.5,10)]}
clf = DecisionTreeClassifier(random_state=25)
GS = GridSearchCV(clf,parameters,cv=10)# 配置了模型，交叉验证
GS.fit(Xtrain,Ytrain)
print("GS.best_params_:",GS.best_params_)
print("Gs_best_score_:",GS.best_score_)
```

在不同max_depth下观察曲线的拟合情况如下图所示，当决策树的深度很大时，会出现过拟合的现象，当深度为3时，模型对训练集和测试集的预测表现较好，故最终可以将max_depth的参数选为3。

![](https://files.mdnice.com/user/25190/e94cec17-afc9-4720-8fb7-7823b7f9316c.png)

使用网格搜索获得的最优参数为

![](https://files.mdnice.com/user/25190/e440fa2f-7045-4a93-9e06-f232471b77a3.png)